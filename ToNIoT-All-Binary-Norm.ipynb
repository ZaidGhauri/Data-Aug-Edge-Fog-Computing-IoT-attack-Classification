{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "def readDataSets():\n",
    "    dataSetFridge = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Fridge.csv')\n",
    "    dataSetGarageDoor = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Garage_Door.csv')\n",
    "    dataSetGPS = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_GPS_Tracker.csv')\n",
    "    dataSetModbus = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Modbus.csv')\n",
    "    #dataSetMotionLight = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Motion_Light.csv')\n",
    "    #dataSetThermostat = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Thermostat.csv')\n",
    "    dataSetWeahter = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Weather.csv')\n",
    "\n",
    "    dataSetFridge['temp_condition'] = dataSetFridge['temp_condition'].str.strip()\n",
    "    dataSetGarageDoor['door_state'] = dataSetGarageDoor['door_state'].str.strip()\n",
    "    #dataSetMotionLight['light_status'] = dataSetMotionLight['light_status'].str.strip()\n",
    "    \n",
    "    dataSetFridge = cutSize(dataSetFridge)\n",
    "    dataSetGarageDoor = cutSize(dataSetGarageDoor)\n",
    "    dataSetGPS = cutSize(dataSetGPS)\n",
    "    dataSetModbus = cutSize(dataSetModbus)\n",
    "    #dataSetMotionLight = cutSize(dataSetMotionLight)\n",
    "    #dataSetThermostat = cutSize(dataSetThermostat)\n",
    "    dataSetWeahter = cutSize(dataSetWeahter)\n",
    "    \n",
    "    \n",
    "    #dataSetRawLoad = pd.concat([dataSetFridge, dataSetGarageDoor, dataSetGPS, dataSetModbus, dataSetMotionLight, dataSetThermostat, dataSetWeahter])\n",
    "    dataSetRawLoad = pd.concat([dataSetFridge, dataSetGarageDoor, dataSetGPS, dataSetModbus, dataSetWeahter])\n",
    "    \n",
    "    print('dataSetRawLoad: ', dataSetRawLoad.shape)\n",
    "    return dataSetRawLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "#categorical_features = ['label', 'door_state','sphone_signal', 'light_status','temp_condition']\n",
    "#quantitative_features = ['FC1_Read_Input_Register','FC2_Read_Discrete_Value','FC3_Read_Holding_Register','FC4_Read_Coil','current_temperature',\n",
    " #                       'fridge_temperature','humidity','latitude','FC4_Read_Coil','longitude',\n",
    "  #                      'motion_status','pressure','temperature','thermostat_status']\n",
    "categorical_features = ['label', 'door_state','sphone_signal', 'temp_condition']\n",
    "quantitative_features = ['FC1_Read_Input_Register','FC2_Read_Discrete_Value','FC3_Read_Holding_Register','FC4_Read_Coil',\n",
    "                        'fridge_temperature','humidity','latitude','FC4_Read_Coil','longitude',\n",
    "                        'pressure','temperature']\n",
    "features = categorical_features + quantitative_features\n",
    "\n",
    "def datapreprocessingShuffle(data):\n",
    "               \n",
    "    # Feature scaling\n",
    "    for i in quantitative_features :\n",
    "            #scaler = StandardScaler()\n",
    "            #data[i] = scaler.fit_transform(data[[i]])\n",
    "            data[i] = data[i]\n",
    "            \n",
    "    # Encoding categorical features    \n",
    "    for i in categorical_features : \n",
    "        labelencoder=LabelEncoder()\n",
    "        data[i]=labelencoder.fit_transform(data[i])   \n",
    "    \n",
    "    data = shuffle(data).reset_index(drop=True) \n",
    "    \n",
    "    Y = data.loc[:,'label']\n",
    "    X = data.drop(['label'],axis=1) \n",
    "    \n",
    "    return(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = readDataSets()\n",
    "# Pre-processing datset\n",
    "datacopy = df.copy()\n",
    "X, y = datapreprocessingShuffle(datacopy) \n",
    "\n",
    "X = X.fillna(X.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,plot_confusion_matrix\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from statistics import mean \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "import time\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "# This method will give the max value item in list\n",
    "def my_max_by_weight(sequence):\n",
    "    if not sequence:\n",
    "        raise ValueError('empty sequence')\n",
    "    maximum = sequence[0]\n",
    "    for item in sequence:\n",
    "        if item[0] > maximum[0]:\n",
    "            maximum = item\n",
    "    return maximum\n",
    "\n",
    "\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def LogisticRegressionAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    cS = np.linspace(1, 100, 100, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for c in cS:\n",
    "        dt = LogisticRegression(C=c, random_state=0)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, c])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = LogisticRegression(C=my_max_by_weight(cValueList)[1], random_state=0)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "\n",
    "# This will give the classification results using KNeighbors  model\n",
    "def KNeighborsAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    n_neighbors = np.linspace(1, 100, 50, endpoint=True, dtype= int)\n",
    "    # object of train and testing arrays from TFIDF vectorizer\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with n_neighbors value for hyper parameter\n",
    "    for n_neighbor in n_neighbors:\n",
    "        dt = KNeighborsClassifier(n_neighbors=n_neighbor,metric='minkowski',p=2)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, n_neighbor])\n",
    "    # using n_neighbors that has most accuracy in the range in the list\n",
    "    dt = KNeighborsClassifier(n_neighbors=my_max_by_weight(cValueList)[1],metric='minkowski',p=2)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def DTAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    max_depths = np.linspace(1, 100, 100, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for max_depth in max_depths:\n",
    "        dt = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, max_depth])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = DecisionTreeClassifier(max_depth=my_max_by_weight(cValueList)[1], random_state=0)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def RFAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    max_depths = np.linspace(1, 100, 100, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for max_depth in max_depths:\n",
    "        dt = RandomForestClassifier(max_depth=max_depth, random_state=0)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, max_depth])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = RandomForestClassifier(max_depth=my_max_by_weight(cValueList)[1], random_state=0)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "def GBAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    max_depths = np.linspace(100, 120, 10, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for max_depth in max_depths:\n",
    "        dt = GradientBoostingClassifier(n_estimators=max_depth)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, max_depth])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = GradientBoostingClassifier(n_estimators=my_max_by_weight(cValueList)[1])\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "def NBAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    max_depths = np.logspace(0,-9, num=10)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for max_depth in max_depths:\n",
    "        dt = GaussianNB(var_smoothing=max_depth)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, max_depth])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = GaussianNB(var_smoothing=my_max_by_weight(cValueList)[1])\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def AdaBoostClassifierAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    #max_depths = np.linspace(1, 10, 10, endpoint=True, dtype= int)\n",
    "    n_trees = [10, 50, 100, 500, 1000, 5000]\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for n in n_trees:\n",
    "        dt = AdaBoostClassifier(n_estimators=n,learning_rate=1)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, n])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = AdaBoostClassifier(n_estimators=my_max_by_weight(cValueList)[1])\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def XGBClassifierAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    max_depths =np.linspace(1, 100, 100, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for max_depth in max_depths:\n",
    "        dt = XGBClassifier(max_depth=max_depth, max_delta_step=0)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, max_depth])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = XGBClassifier(max_depth=my_max_by_weight(cValueList)[1], max_delta_step=0)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def SVCAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    cS = np.linspace(1, 100, 100, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for c in cS:\n",
    "        dt = SVC(C = c , kernel=\"rbf\",random_state=0)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, c])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = SVC(C=my_max_by_weight(cValueList)[1], kernel=\"rbf\",random_state=0)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def LDAAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    cS = ['svd', 'lsqr', 'eigen']\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for c in cS:\n",
    "        dt = LinearDiscriminantAnalysis(solver = c)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, c])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = LinearDiscriminantAnalysis(solver=my_max_by_weight(cValueList)[1])\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy}\n",
    "    return resultObj\n",
    "\n",
    "def SoftVotingAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    models = list()\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeRegressor()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('MLP', MLPClassifier()))\n",
    "    models.append(('DT', DecisionTreeClassifier()))\n",
    "    models.append(('RF', RandomForestClassifier()))\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('SVM', svm.SVC()))\n",
    "    models.append(('AdaBoost', AdaBoostClassifier()))\n",
    "    models.append(('GradientBoosting', GradientBoostingClassifier()))\n",
    "    models.append(('XGB', XGBClassifier()))\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for c in models:\n",
    "        dt = VotingClassifier(estimators=c, voting='soft')\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, c])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = VotingClassifier(estimators=my_max_by_weight(cValueList)[1], voting='soft')\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "\n",
    "lr = LogisticRegressionAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"LR\", lr)\n",
    "knn = KNeighborsAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"KNN\", knn)\n",
    "dt = DTAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"DT\", dt)\n",
    "rf = RFAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"RF\", rf)\n",
    "gb = GBAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"GB\", gb)\n",
    "nb = NBAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"NB\", nb)\n",
    "ada = AdaBoostClassifierAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"ADA\", ada)\n",
    "xgb = XGBClassifierAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"XGB\", xgb)\n",
    "svm = SVCAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"SVM\", svm)\n",
    "lda = LDAAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"LDA\", lda)\n",
    "sv = SoftVotingAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"gb\", sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "\n",
    "lr = LogisticRegressionAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"LR\", lr)\n",
    "knn = KNeighborsAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"KNN\", knn)\n",
    "dt = DTAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"DT\", dt)\n",
    "rf = RFAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"RF\", rf)\n",
    "gb = GBAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"GB\", gb)\n",
    "nb = NBAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"NB\", nb)\n",
    "ada = AdaBoostClassifierAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"ADA\", ada)\n",
    "xgb = XGBClassifierAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"XGB\", xgb)\n",
    "svm = SVCAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"SVM\", svm)\n",
    "lda = LDAAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"LDA\", lda)\n",
    "sv = SoftVotingAccuracy(X_train, X_test, y_train, y_test)\n",
    "print(\"gb\", sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = readDataSets()\n",
    "# Pre-processing datset\n",
    "datacopy = df.copy()\n",
    "X, y = datapreprocessingShuffle(datacopy) \n",
    "\n",
    "X = X.fillna(X.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection  import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "# We use a utility to generate artificial classification data.\n",
    "X = X.values\n",
    "y = y.values\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "sss.get_n_splits(X, y)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    lr = LogisticRegressionAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"LR\", lr)\n",
    "    knn = KNeighborsAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"KNN\", knn)\n",
    "    dt = DTAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"DT\", dt)\n",
    "    rf = RFAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"RF\", rf)\n",
    "    gb = GBAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"GB\", gb)\n",
    "    nb = NBAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"NB\", nb)\n",
    "    ada = AdaBoostClassifierAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"ADA\", ada)\n",
    "    xgb = XGBClassifierAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"XGB\", xgb)\n",
    "    svm = SVCAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"SVM\", svm)\n",
    "    lda = LDAAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"LDA\", lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values\n",
    "y = y.values\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "sss.get_n_splits(X, y)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
    "sss.get_n_splits(X, y)\n",
    "\n",
    "print(sss)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "test_size=0.20, random_state=0)\n",
    "\n",
    "dt = DecisionTreeRegressor(random_state=0, criterion=\"mae\")\n",
    "dt_fit = dt.fit(X_train, y_train)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "dt_scores = cross_val_score(dt_fit, X_train, y_train, cv = kf)\n",
    "print(\"mean cross validation score 10 : {}\".format(np.mean(dt_scores)))\n",
    "print(\"score without cv 10 : {}\".format(dt_fit.score(X_train, y_train)))\n",
    "\n",
    "kf4 = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "dt_scores1 = cross_val_score(dt_fit, X_train, y_train, cv = kf4)\n",
    "print(\"mean cross validation score 4 : {}\".format(np.mean(dt_scores)))\n",
    "print(\"score without cv 4 : {}\".format(dt_fit.score(X_train, y_train)))\n",
    "\n",
    "\n",
    "# on the test or hold-out set\n",
    "from sklearn.metrics import r2_score\n",
    "print(f'Scores for each fold are: {r2_score(y_test, dt_fit.predict(X_test))}')\n",
    "print(f'Average score: {\"{:.2f}\".format(dt_fit.score(X_test, y_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "score = cross_val_score(DecisionTreeClassifier(max_depth= 45, random_state= 42), X, y, cv= kf, scoring=\"accuracy\")\n",
    "print(f'Scores for each fold are: {score}')\n",
    "print(f'Average score: {\"{:.2f}\".format(score.mean())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_max_by_weight(sequence):\n",
    "    if not sequence:\n",
    "        raise ValueError('empty sequence')\n",
    "    maximum = sequence[0]\n",
    "    for item in sequence:\n",
    "        if item[0] > maximum[0]:\n",
    "            maximum = item\n",
    "    return maximum\n",
    "\n",
    "def rmse(score):\n",
    "    rmse = np.sqrt(-score)\n",
    "    print(f'rmse= {\"{:.2f}\".format(rmse)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = np.linspace(1, 100, 100, endpoint=True, dtype= int)\n",
    "cValueList = []\n",
    "\n",
    "for val in max_depths:\n",
    "    score = cross_val_score(DecisionTreeClassifier(max_depth= val, random_state= 42), X, y, cv= kf, scoring=\"accuracy\")\n",
    "    cValueList.append([score, val])\n",
    "    print(f'Scores for each fold are: {score}')\n",
    "    print(f'Average score: {\"{:.2f}\".format(score.mean())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cross_val_score(DecisionTreeRegressor(random_state= 42), X, y, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "print(f'Scores for each fold: {score}')\n",
    "rmse(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "test_size=0.20, random_state=0)\n",
    "\n",
    "dt = DecisionTreeRegressor(random_state=0, criterion=\"mae\")\n",
    "dt_fit = dt.fit(X_train, y_train)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "dt_scores = cross_val_score(dt_fit, X_train, y_train, cv = kf)\n",
    "print(\"mean cross validation score: {}\".format(np.mean(dt_scores)))\n",
    "print(\"score without cv: {}\".format(dt_fit.score(X_train, y_train)))\n",
    "\n",
    "# on the test or hold-out set\n",
    "from sklearn.metrics import r2_score\n",
    "print(f'Scores for each fold are: {r2_score(y_test, dt_fit.predict(X_test))}')\n",
    "print(f'Average score: {\"{:.2f}\".format(dt_fit.score(X_test, y_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "test_size=0.20, random_state=0)\n",
    "\n",
    "\n",
    "max_depths = np.linspace(1, 100, 100, endpoint=True, dtype= int)\n",
    "cValueList = []\n",
    "\n",
    "for val in max_depths:\n",
    "    dt = DecisionTreeClassifier(max_depth= val, random_state=0)\n",
    "    dt_fit = dt.fit(X_train, y_train)\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(dt, X, y, cv= kf, scoring=\"accuracy\")\n",
    "\n",
    "    \n",
    "    dt_scores = cross_val_score(dt_fit, X_train, y_train, cv = kf)\n",
    "    print(\"mean cross validation score: {}\".format(np.mean(dt_scores)))\n",
    "    print(\"score without cv: {}\".format(dt_fit.score(X_train, y_train)))\n",
    "\n",
    "    # on the test or hold-out set\n",
    "    from sklearn.metrics import r2_score\n",
    "    print(r2_score(y_test, dt_fit.predict(X_test)))\n",
    "    print(dt_fit.score(X_test, y_test))\n",
    "    print(f'Scores for each fold are: {score}')\n",
    "    print(f'Average score: {\"{:.2f}\".format(score.mean())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "test_size=0.20, random_state=0)\n",
    "depth2=[]\n",
    "for i in range(1,20):\n",
    "    regressor2 = DecisionTreeRegressor(random_state=0, criterion=\"mae\", max_depth=i)\n",
    "    model2 = regressor2.fit(X_train, y_train)\n",
    "    # Perform 5-fold cross validation \n",
    "    model2_scores = cross_val_score(model2, X_train, y_train, cv = 5, scoring='r2')\n",
    "    print(\"mean cross validation score: {}\".format(np.mean(model2_scores)))\n",
    "##########\n",
    "    #Predict the response for test dataset\n",
    "    y_pred2 = model2.predict(X_test)\n",
    "    depth2.append(mean_squared_error(y_te, y_pred2))\n",
    "print(depth2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "####### K-fold cross validation #######\n",
    "# Defining a custom function to calculate accuracy\n",
    "# Make sure there are no zeros in the Target variable if you are using MAPE\n",
    "def Accuracy_Score(orig,pred):\n",
    "    MAPE = np.mean(100 * (np.abs(orig-pred)/orig))\n",
    "    #print('#'*70,'Accuracy:', 100-MAPE)\n",
    "    return(100-MAPE)\n",
    " \n",
    "# Custom Scoring MAPE calculation\n",
    "from sklearn.metrics import make_scorer\n",
    "custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)\n",
    " \n",
    "# Importing cross validation function from sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "ColumnNames=['Hours','Calories', 'Weight']\n",
    "DataValues=[[  1.0,   2500,   95],\n",
    "             [  2.0,   2000,   85],\n",
    "             [  2.5,   1900,   83],\n",
    "             [  3.0,   1850,   81],\n",
    "             [  3.5,   1600,   80],\n",
    "             [  4.0,   1500,   78],\n",
    "             [  5.0,   1500,   77],\n",
    "             [  5.5,   1600,   80],\n",
    "             [  6.0,   1700,   75],\n",
    "             [  6.5,   1500,   70]]\n",
    "#Create the Data Frame\n",
    "GymData=pd.DataFrame(data=DataValues,columns=ColumnNames)\n",
    "GymData.head()\n",
    " \n",
    "#Separate Target Variable and Predictor Variables\n",
    "TargetVariable='Weight'\n",
    "Predictors=['Hours','Calories']\n",
    "X1=GymData[Predictors].values\n",
    "y1=GymData[TargetVariable].values\n",
    "\n",
    " \n",
    "###### Single Decision Tree Regression in Python #######\n",
    "from sklearn import tree\n",
    "#choose from different tunable hyper parameters\n",
    "RegModel = tree.DecisionTreeRegressor(max_depth=3,criterion='mse')\n",
    " \n",
    "# Running 10-Fold Cross validation on a given algorithm\n",
    "# Passing full data X and y because the K-fold will split the data and automatically choose train/test\n",
    "Accuracy_Values=cross_val_score(RegModel, X1 , y1, cv=10, scoring=custom_Scoring)\n",
    "print('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\n",
    "print('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GymData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, \n",
    "test_size=0.20, random_state=0)\n",
    "\n",
    "n_features = X.shape[1]\n",
    "n_samples = X.shape[0]\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeRegressor(random_state=1), cv=3, n_jobs=-1, verbose=5,\n",
    "                    param_grid ={\n",
    "                    'max_depth': [None,1,2,3,4,5,6,7],\n",
    "                    'max_features': [None, 'sqrt', 'auto', 'log2', 0.3,0.5,0.7, n_features//2, n_features//3, ],\n",
    "                    'min_samples_split': [2,0.3,0.5, n_samples//2, n_samples//3, n_samples//5],\n",
    "                    'min_samples_leaf':[1, 0.3,0.5, n_samples//2, n_samples//3, n_samples//5]},\n",
    "                    )\n",
    "\n",
    "grid.fit(X_train, Y_train)\n",
    "print('Train R^2 Score : %.3f'%grid.best_estimator_.score(X_train, Y_train))\n",
    "print('Test R^2 Score : %.3f'%grid.best_estimator_.score(X_test, Y_test))\n",
    "print('Best R^2 Score Through Grid Search : %.3f'%grid.best_score_)\n",
    "print('Best Parameters : ',grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "for algo in algorithms:\n",
    "    score = cross_val_score(LogisticRegression(max_iter= 10, solver= algo, random_state= 42), X, y, cv= kf, scoring=\"accuracy\")\n",
    "    print(f'Average score({algo}): {\"{:.3f}\".format(score.mean())}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models = []\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('MLP', MLPClassifier()))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('AdaBoost', AdaBoostClassifier()))\n",
    "models.append(('GradientBoosting', GradientBoostingClassifier()))\n",
    "models.append(('XGB', XGBClassifier()))\n",
    "models.append(('SVM', svm.SVC()))\n",
    "\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)\n",
    "           #,'auc-score' : make_scorer(roc_auc_score)\n",
    "          }\n",
    "\n",
    "\n",
    "\n",
    "steps = [('pca', PCA(n_components=10)), ('cart', DecisionTreeRegressor())]\n",
    "model = Pipeline(steps=steps)\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "start = time.time()\n",
    "cv_results = cross_validate(model, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "#print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "print(end - start, \"seconds\")\n",
    "#print(cv_results)\n",
    "print(np.mean(cv_results['test_accuracy']))\n",
    "print(np.mean(cv_results['test_precision']))\n",
    "print(np.mean(cv_results['test_recall']))\n",
    "print(np.mean(cv_results['test_f1_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    start = time.time()\n",
    "    kfold = StratifiedKFold(n_splits=4, random_state=1, shuffle=True)\n",
    "    cv_results = cross_validate(model, X, y, cv=kfold, scoring=scoring)\n",
    "    end = time.time()\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n",
    "    print(name)\n",
    "    print(end - start, \"seconds\")\n",
    "    #print(cv_results)\n",
    "    print(np.mean(cv_results['test_accuracy']))\n",
    "    print(np.mean(cv_results['test_precision']))\n",
    "    print(np.mean(cv_results['test_recall']))\n",
    "    print(np.mean(cv_results['test_f1_score']))\n",
    "    #print(np.mean(cv_results['test_auc-score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Fridge.csv').ilco[]\n",
    "pd.DataFrame(d['label'].value_counts())[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def funcTransform(d):\n",
    "    d1 = d.iloc[:,:-1]\n",
    "    \n",
    "    dN =pd.DataFrame()\n",
    "    label=LabelEncoder()\n",
    "    for c in  d1.columns:\n",
    "        if(d[c].dtype=='object'):\n",
    "            dN[c]=label.fit_transform(d1[c])\n",
    "        else:\n",
    "            dN[c]=d1[c]\n",
    "            \n",
    "    c = dN.columns\n",
    "    ff_arr = preprocessing.normalize(dN)\n",
    "    df1 = pd.DataFrame(data=ff_arr, columns=c)\n",
    "    df1['label'] = d['label']\n",
    "    return df1\n",
    "\n",
    "def funcTransform1(d):\n",
    "    dN =pd.DataFrame()\n",
    "    label=LabelEncoder()\n",
    "    for c in  d.columns:\n",
    "        if(d[c].dtype=='object'):\n",
    "            dN[c]=label.fit_transform(d[c])\n",
    "        else:\n",
    "            dN[c]=d[c]\n",
    "            \n",
    "    return dN\n",
    "\n",
    "def cutSize(df):\n",
    "    newdf = df[df['label'] == 0].iloc[0:5000,:]\n",
    "    n = pd.concat([newdf, df[df['label'] == 1].iloc[0:5000,:]]).sample(frac=1).reset_index(drop=True)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import datetime\n",
    "\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "def readDataSets():\n",
    "    dataSetFridge = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Fridge.csv')    \n",
    "    dataSetGarageDoor = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Garage_Door.csv')\n",
    "    dataSetGPS = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_GPS_Tracker.csv')\n",
    "    dataSetModbus = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Modbus.csv')\n",
    "    dataSetMotionLight = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Motion_Light.csv')\n",
    "    dataSetThermostat = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Thermostat.csv')\n",
    "    dataSetWeahter = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Weather.csv')\n",
    "\n",
    "    dataSetFridge['temp_condition'] = dataSetFridge['temp_condition'].str.strip()\n",
    "    dataSetGarageDoor['door_state'] = dataSetGarageDoor['door_state'].str.strip()\n",
    "    dataSetMotionLight['light_status'] = dataSetMotionLight['light_status'].str.strip()\n",
    "\n",
    "    dataSetFridge = funcTransform(cutSize(dataSetFridge))\n",
    "    dataSetGarageDoor = funcTransform(cutSize(dataSetGarageDoor))\n",
    "    dataSetGPS = funcTransform(cutSize(dataSetGPS))\n",
    "    dataSetModbus = funcTransform(cutSize(dataSetModbus))\n",
    "    dataSetMotionLight = funcTransform(cutSize(dataSetMotionLight))\n",
    "    dataSetThermostat = funcTransform(cutSize(dataSetThermostat))\n",
    "    dataSetWeahter = funcTransform(cutSize(dataSetWeahter))\n",
    "\n",
    "    dataSetRawLoad = pd.concat([dataSetFridge, dataSetGarageDoor, dataSetGPS, dataSetModbus, dataSetMotionLight, dataSetThermostat, dataSetWeahter])\n",
    "    \n",
    "    print('dataSetRawLoad: ', dataSetRawLoad.shape)\n",
    "    return dataSetRawLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = readDataSets()\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "df.head()\n",
    "\n",
    "a = pd.DataFrame(df['label'].value_counts())[:]\n",
    "a.plot(kind='pie', subplots=True, figsize=(5, 5))\n",
    "plt.title('ToN-IoT Dataset Attacks')\n",
    "plt.legend(loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df['label'].value_counts())[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loc[:,'label']\n",
    "X = df.drop(['label'],axis=1) \n",
    "X = X.fillna(X.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = df[df['label'] == 0].iloc[0:50000,:]\n",
    "n = pd.concat([newdf, df[df['label'] == 1].iloc[0:50000,:]]).sample(frac=1).reset_index(drop=True)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = n.loc[:,'label']\n",
    "X = n.drop(['label'],axis=1) \n",
    "X = X.fillna(n.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "import time\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeRegressor()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('MLP', MLPClassifier()))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('AdaBoost', AdaBoostClassifier()))\n",
    "models.append(('GradientBoosting', GradientBoostingClassifier()))\n",
    "models.append(('XGB', XGBClassifier()))\n",
    "\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)\n",
    "           #,'auc-score' : make_scorer(roc_auc_score)\n",
    "          }\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    start = time.time()\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    cv_results = cross_validate(model, X, y, cv=kfold, scoring=scoring)\n",
    "    end = time.time()\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n",
    "    print(name)\n",
    "    print(end - start, \"seconds\")\n",
    "    #print(cv_results)\n",
    "    print(np.mean(cv_results['test_accuracy']))\n",
    "    print(np.mean(cv_results['test_precision']))\n",
    "    print(np.mean(cv_results['test_recall']))\n",
    "    print(np.mean(cv_results['test_f1_score']))\n",
    "    #print(np.mean(cv_results['test_auc-score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('MLP', MLPClassifier()))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('AdaBoost', AdaBoostClassifier()))\n",
    "models.append(('GradientBoosting', GradientBoostingClassifier()))\n",
    "models.append(('XGB', XGBClassifier()))\n",
    "models.append(('SVM', svm.SVC()))\n",
    "\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)\n",
    "           #,'auc-score' : make_scorer(roc_auc_score)\n",
    "          }\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    start = time.time()\n",
    "    kfold = StratifiedKFold(n_splits=4, random_state=1, shuffle=True)\n",
    "    cv_results = cross_validate(model, X, y, cv=kfold, scoring=scoring)\n",
    "    end = time.time()\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n",
    "    print(name)\n",
    "    print(end - start, \"seconds\")\n",
    "    #print(cv_results)\n",
    "    print(np.mean(cv_results['test_accuracy']))\n",
    "    print(np.mean(cv_results['test_precision']))\n",
    "    print(np.mean(cv_results['test_recall']))\n",
    "    print(np.mean(cv_results['test_f1_score']))\n",
    "    #print(np.mean(cv_results['test_auc-score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('MLP', MLPClassifier()))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('AdaBoost', AdaBoostClassifier()))\n",
    "models.append(('GradientBoosting', GradientBoostingClassifier()))\n",
    "models.append(('XGB', XGBClassifier()))\n",
    "models.append(('SVM', svm.SVC()))\n",
    "\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    cv_results = cross_validate(model, X, y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n",
    "    print(name)\n",
    "    #print(cv_results)\n",
    "    print(np.mean(cv_results['test_accuracy']))\n",
    "    print(np.mean(cv_results['test_precision']))\n",
    "    print(np.mean(cv_results['test_recall']))\n",
    "    print(np.mean(cv_results['test_f1_score']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
