{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cGAN():\n",
    "    def __init__(self):\n",
    "        self.latent_dim = 120\n",
    "        self.out_shape = 120\n",
    "        self.num_classes = 2\n",
    "        self.clip_value = 0.01\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        #optimizer = RMSprop(lr=0.00005)\n",
    "\n",
    "        # build discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        # build generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # generating new data samples\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        gen_samples = self.generator([noise, label])\n",
    "\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # passing gen samples through disc. \n",
    "        valid = self.discriminator([gen_samples, label])\n",
    "\n",
    "        # combining both models\n",
    "        self.combined = Model([noise, label], valid)\n",
    "        self.combined.compile(loss=['binary_crossentropy'],\n",
    "                              optimizer=optimizer,\n",
    "                             metrics=['accuracy'])\n",
    "        self.combined.summary()\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self):\n",
    "        init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128, input_dim=self.latent_dim))\n",
    "        #model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        model.add(Dense(256))\n",
    "        #model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        model.add(Dense(512))\n",
    "        #model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        model.add(Dense(self.out_shape, activation='tanh'))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "        \n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        gen_sample = model(model_input)\n",
    "\n",
    "        return Model([noise, label], gen_sample, name=\"Generator\")\n",
    "\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.out_shape, kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Dense(256, kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        model.add(Dense(128, kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "        \n",
    "        gen_sample = Input(shape=(self.out_shape,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.out_shape)(label))\n",
    "\n",
    "        model_input = multiply([gen_sample, label_embedding])\n",
    "        validity = model(model_input)\n",
    "\n",
    "        return Model(inputs=[gen_sample, label], outputs=validity, name=\"Discriminator\")\n",
    "\n",
    "\n",
    "    def train(self, X_train, y_train, pos_index, neg_index, epochs, batch_size=32, sample_interval=50):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            #  Train Discriminator with 8 sample from postivite class and rest with negative class\n",
    "            idx1 = np.random.choice(pos_index, 8)\n",
    "            idx0 = np.random.choice(neg_index, batch_size-8)\n",
    "            idx = np.concatenate((idx1, idx0))\n",
    "            samples, labels = X_train[idx], y_train[idx]\n",
    "            samples, labels = shuffle(samples, labels)\n",
    "            # Sample noise as generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_samples = self.generator.predict([noise, labels])\n",
    "\n",
    "            # label smoothing\n",
    "            if epoch < epochs//1.5:\n",
    "                valid_smooth = (valid+0.1)-(np.random.random(valid.shape)*0.1)\n",
    "                fake_smooth = (fake-0.1)+(np.random.random(fake.shape)*0.1)\n",
    "            else:\n",
    "                valid_smooth = valid \n",
    "                fake_smooth = fake\n",
    "                \n",
    "            # Train the discriminator\n",
    "            self.discriminator.trainable = True\n",
    "            d_loss_real = self.discriminator.train_on_batch([samples, labels], valid_smooth)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([gen_samples, labels], fake_smooth)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train Generator\n",
    "            # Condition on labels\n",
    "            self.discriminator.trainable = False\n",
    "            sampled_labels = np.random.randint(0, 2, batch_size).reshape(-1, 1)\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            if (epoch+1)%sample_interval==0:\n",
    "                print (f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}] [G loss: {g_loss}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
