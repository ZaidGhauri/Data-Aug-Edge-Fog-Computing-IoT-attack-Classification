{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "ioTInTData = pd.read_csv(\"data/IoTID20/IoT_Network_Intrusion_Dataset_ToCombine.csv\" , low_memory=False)\n",
    "ioTInTData = ioTInTData[ioTInTData['Cat'] != 'Mirai']\n",
    "ioTInTData = ioTInTData.iloc[:,:57]\n",
    "print('ioTInTData', ioTInTData.shape)\n",
    "\n",
    "#ioTInTData = ioTInTData.fillna(np.nan, inplace=True)\n",
    "ioTInTData.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "ioTInTData.dropna(inplace=True)\n",
    "ioTInTData.fillna(0)\n",
    "ioTInTData.shape\n",
    "\n",
    "def readDataSets():\n",
    "    dataSetFridge = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Fridge.csv')\n",
    "    dataSetGarageDoor = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Garage_Door.csv')\n",
    "    dataSetGPS = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_GPS_Tracker.csv')\n",
    "    dataSetModbus = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Modbus.csv')\n",
    "    dataSetMotionLight = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Motion_Light.csv')\n",
    "    dataSetThermostat = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Thermostat.csv')\n",
    "    dataSetWeahter = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Weather.csv')\n",
    "\n",
    "    dataSetFridge['temp_condition'] = dataSetFridge['temp_condition'].str.strip()\n",
    "    dataSetGarageDoor['door_state'] = dataSetGarageDoor['door_state'].str.strip()\n",
    "    dataSetMotionLight['light_status'] = dataSetMotionLight['light_status'].str.strip()\n",
    "    dataSetRawLoad = pd.concat([dataSetFridge, dataSetGarageDoor, dataSetGPS, dataSetModbus, dataSetMotionLight, dataSetThermostat, dataSetWeahter])\n",
    "    return dataSetRawLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = readDataSets()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "categorical_features = ['door_state','sphone_signal', 'light_status','temp_condition']\n",
    "quantitative_features = ['FC1_Read_Input_Register','FC2_Read_Discrete_Value','FC3_Read_Holding_Register','FC4_Read_Coil','current_temperature',\n",
    "                        'fridge_temperature','humidity','latitude','FC4_Read_Coil','longitude',\n",
    "                        'motion_status','pressure','temperature','thermostat_status']\n",
    "features = categorical_features + quantitative_features\n",
    "\n",
    "\n",
    "def datapreprocessingShuffle(data):\n",
    "    scaler = StandardScaler()            \n",
    "    # Feature scaling\n",
    "    for i in data.columns:\n",
    "        if(data[i].name !='Label'):\n",
    "            data[i] = scaler.fit_transform(data[[i]])\n",
    "        else:\n",
    "            data[i]=data[i]\n",
    "            \n",
    "    data = shuffle(data).reset_index(drop=True) \n",
    "    return data\n",
    "\n",
    "\n",
    "def datapreprocessingShuffle2(data):\n",
    "               \n",
    "    # Feature scaling\n",
    "    for i in quantitative_features :\n",
    "            scaler = StandardScaler()\n",
    "            data[i] = scaler.fit_transform(data[[i]])\n",
    "            \n",
    "    # Encoding categorical features    \n",
    "    for i in categorical_features :\n",
    "        labelencoder=LabelEncoder()\n",
    "        data[i]=labelencoder.fit_transform(data[i])   \n",
    "    \n",
    "    data = shuffle(data).reset_index(drop=True) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing datset\n",
    "datacopy1 = ioTInTData.copy()\n",
    "#datacopy1 = datacopy1.drop(['Cat'],axis=1) \n",
    "datacopy2 = df.copy()\n",
    "datacopy1['Label'] = datacopy1.iloc[:,56:57].replace('Normal', '0').replace('Anomaly', '1').astype('int')\n",
    "data1 = datapreprocessingShuffle(datacopy1)\n",
    "\n",
    "data2 = datacopy2.rename(columns={\"label\": \"Label\"})\n",
    "data2 = datapreprocessingShuffle2(data2) \n",
    "# fill na with mean for ToN_IoT dataset\n",
    "data2 = data2.fillna(data2.mean())\n",
    "print(data1.shape)\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of columns\n",
    "cols = list(data2)\n",
    "# move the column to head of list using index, pop and insert\n",
    "cols.insert(18, cols.pop(cols.index('Label')))\n",
    "\n",
    "# use ix to reorder\n",
    "data2 = data2.ix[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "count =  data2.loc[data2['Label'] == 0] #no attack\n",
    "print(count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  170033  |   d2a 170033\n",
    "----------------------- = 326,152\n",
    "  d1a 156119  |   156119\n",
    "-----------------------\n",
    "   40073  |   d2na 40073\n",
    "-----------------------\n",
    "  d1na 245000  |  245000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for false labels\n",
    "#ganData = data2.loc[data2['Label'] == 1] #attack\n",
    "\n",
    "# for false labels\n",
    "#ganData = data1.loc[data1['Label'] == 1] #attack\n",
    "\n",
    "# for false labels\n",
    "#ganData = data2.loc[data2['Label'] == 0] #no attack\n",
    "\n",
    "# for false labels\n",
    "ganData = data1.loc[data1['Label'] == 0] #no attack\n",
    "\n",
    "\n",
    "\n",
    "import GAN_171103\n",
    "import importlib\n",
    "importlib.reload(GAN_171103) # For reloading after making changes\n",
    "from GAN_171103 import *\n",
    "\n",
    "\n",
    " # 32 # needs to be ~data_dim\n",
    "base_n_count = 128 # 128\n",
    "\n",
    "nb_steps = 500 + 1 # 50000 # Add one for logging of the last interval\n",
    "batch_size = 128 # 64\n",
    "\n",
    "k_d = 1  # number of critic network updates per adversarial training step\n",
    "k_g = 1  # number of generator network updates per adversarial training step\n",
    "critic_pre_train_steps = 100 # 100  # number of steps to pre-train the critic before starting adversarial training\n",
    "log_interval = 100 # 100  # interval (in steps) at which to log loss summaries and save plots of image samples to disc\n",
    "learning_rate = 5e-4 # 5e-5\n",
    "data_dir = 'cache/'\n",
    "generator_model_path, discriminator_model_path, loss_pickle_path = None, None, None\n",
    "\n",
    "# show = False\n",
    "show = True \n",
    "\n",
    "# train = create_toy_spiral_df(1000)\n",
    "# train = create_toy_df(n=1000,n_dim=2,n_classes=4,seed=0)\n",
    "train = ganData.copy().reset_index(drop=True) # fraud only with labels from classification\n",
    "\n",
    "\n",
    "# train = pd.get_dummies(train, columns=['Class'], prefix='Class', drop_first=True)\n",
    "label_cols = [ i for i in train.columns if 'Label' in i ]\n",
    "data_cols = [ i for i in train.columns if i not in label_cols ]\n",
    "train[ data_cols ] = train[ data_cols ] / 10 # scale to random noise size, one less thing to learn\n",
    "train_no_label = train[ data_cols ]\n",
    "\n",
    "\n",
    "rand_dim = len(data_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training the vanilla GAN and CGAN architectures\n",
    "\n",
    "k_d = 1  # number of critic network updates per adversarial training step\n",
    "learning_rate = 5e-4 # 5e-5\n",
    "arguments = [rand_dim, nb_steps, batch_size, \n",
    "             k_d, k_g, critic_pre_train_steps, log_interval, learning_rate, base_n_count,\n",
    "            data_dir, generator_model_path, discriminator_model_path, loss_pickle_path, show ]\n",
    "\n",
    "adversarial_training_GAN(arguments, train_no_label, data_cols ) # GAN\n",
    "#adversarial_training_GAN(arguments, train, data_cols=data_cols, label_cols=label_cols ) # CGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 17\n",
    "\n",
    "train = ganData.copy().reset_index(drop=True) # fraud only with labels from classification\n",
    "\n",
    "# train = pd.get_dummies(train, columns=['Class'], prefix='Class', drop_first=True)\n",
    "label_cols = [ train.columns[-1]  ]\n",
    "data_cols = [ i for i in train.columns if i not in label_cols ]\n",
    "#train[ data_cols ] = train[ data_cols ] / 10 # scale to random noise size, one less thing to learn\n",
    "train_no_label = train[ data_cols ]\n",
    "\n",
    "data_dim = len(data_cols)\n",
    "label_dim = len(label_cols)\n",
    "#if label_dim > 0: with_class = True\n",
    "np.random.seed(seed)\n",
    "\n",
    "# define network models\n",
    "\n",
    "generator_model, discriminator_model, combined_model = define_models_GAN(rand_dim, data_dim, base_n_count)\n",
    "generator_model.load_weights('cache/GAN_generator_model_weights_step_500.h5')\n",
    "\n",
    "\n",
    "# Now generate some new data\n",
    "test_size = len(train) # Equal to all of the fraud cases\n",
    "\n",
    "x = get_data_batch(train_no_label, test_size, seed=3)\n",
    "z = np.random.normal(size=(test_size, rand_dim))\n",
    "g_z = generator_model.predict(z)\n",
    "\n",
    " \n",
    "\n",
    "# =============================================================================\n",
    "# check data\n",
    "# =============================================================================\n",
    "df2=pd.DataFrame(np.rint(np.abs(g_z)),columns=ganData.columns[:-1])\n",
    "df2=df2.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "def define_models_GAN(rand_dim, data_dim, base_n_count, type=None):\n",
    "    generator_input_tensor = layers.Input(shape=(rand_dim, ))\n",
    "    generated_image_tensor = generator_network(generator_input_tensor, data_dim, base_n_count)\n",
    "\n",
    "    generated_or_real_image_tensor = layers.Input(shape=(data_dim,))\n",
    "    \n",
    "    if type == 'Wasserstein':\n",
    "        discriminator_output = critic_network(generated_or_real_image_tensor, data_dim, base_n_count)\n",
    "    else:\n",
    "        discriminator_output = discriminator_network(generated_or_real_image_tensor, data_dim, base_n_count)\n",
    "\n",
    "    generator_model = models.Model(inputs=[generator_input_tensor], outputs=[generated_image_tensor], name='generator')\n",
    "    discriminator_model = models.Model(inputs=[generated_or_real_image_tensor],\n",
    "                                       outputs=[discriminator_output],\n",
    "                                       name='discriminator')\n",
    "\n",
    "    combined_output = discriminator_model(generator_model(generator_input_tensor))\n",
    "    combined_model = models.Model(inputs=[generator_input_tensor], outputs=[combined_output], name='combined')\n",
    "    \n",
    "    return generator_model, discriminator_model, combined_model\n",
    "\n",
    "def generate_data_gan(n_samples,Xdf,path_model,rand_dim=32):\n",
    "    generator_model, discriminator_model, combined_model= define_models_GAN(rand_dim,data_dim=len(Xdf.columns),base_n_count=128)\n",
    "    generator_model.load_weights(path_model)\n",
    "    z = np.random.normal(size=(n_samples, rand_dim))\n",
    "    g_z = generator_model.predict(z)\n",
    "    \n",
    "    newdf = pd.DataFrame()\n",
    "    for i, col in enumerate(Xdf.columns):\n",
    "\n",
    "        if Xdf[col].dtype == 'int32' or Xdf[col].dtype == 'int64':\n",
    "            newdf[col] = np.rint(np.abs(g_z[:,i])).astype(int)    \n",
    "        elif Xdf[col].dtype == 'float_':\n",
    "            newdf[col] = np.abs(g_z[:,i]) \n",
    "    return newdf\n",
    "\n",
    "\n",
    "\n",
    "#data2 = 401119\n",
    "#data1 = 119698\n",
    "\n",
    "#data1 Attack (92172)          | data2 GAN attack (~170033)\n",
    "#------------------------------|------------------\n",
    "#data1 GAN Attack (~156119)    | data2 attack (156119)\n",
    "#------------------------------|------------------\n",
    "#data1 No Attack (27526)       | data2 GAN No attack (~40073)\n",
    "#------------------------------|------------------\n",
    "#data1 GAN No Attack (~245000) | data2 No attack (245000)\n",
    "#170033\n",
    "n_samples=245000 \n",
    "\n",
    "# data2 True\n",
    "#n_samples=45485\n",
    "\n",
    "\n",
    "Xdf=ganData.iloc[:,:-1]    \n",
    "path_model='cache/GAN_generator_model_weights_step_500.h5'\n",
    "\n",
    "#data2GA=generate_data_gan(n_samples,Xdf,path_model,rand_dim=len(data_cols))  \n",
    "#data2GA.shape\n",
    "\n",
    "#data1GA=generate_data_gan(n_samples,Xdf,path_model,rand_dim=len(data_cols))  \n",
    "#data1GA.shape\n",
    "\n",
    "#data2GNa=generate_data_gan(n_samples,Xdf,path_model,rand_dim=len(data_cols))  \n",
    "#data2GNa.shape\n",
    "\n",
    "data1GNa=generate_data_gan(n_samples,Xdf,path_model,rand_dim=len(data_cols))  \n",
    "data1GNa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#row1\n",
    "tData1 = data1.loc[data1['Label'] == 1] \n",
    "tData1 = tData1.iloc[:,:-1]\n",
    "tData1 = tData1.reset_index(drop=True)\n",
    "row1A = pd.concat([tData1, data2GA], axis = 1).reset_index(drop=True)\n",
    "row1A.shape\n",
    "\n",
    "#row2\n",
    "tData2 = data2.loc[data2['Label'] == 1] \n",
    "tData2 = tData2.iloc[:,:-1]\n",
    "tData2 = tData2.reset_index(drop=True)\n",
    "row2A = pd.concat([data1GA, tData2], axis = 1).reset_index(drop=True)\n",
    "row2A.shape\n",
    "\n",
    "#row3\n",
    "tData1False = data1.loc[data1['Label'] == 0] \n",
    "tData1False = tData1False.iloc[:,:-1]\n",
    "tData1False = tData1False.reset_index(drop=True)\n",
    "row3A = pd.concat([tData1False, data2GNa], axis = 1).reset_index(drop=True)\n",
    "row3A.shape\n",
    "\n",
    "#row4\n",
    "tData2False = data2.loc[data2['Label'] == 0] \n",
    "tData2False = tData2False.iloc[:,:-1]\n",
    "tData2False = tData2False.reset_index(drop=True)\n",
    "row4A = pd.concat([data1GNa, tData2False], axis = 1).reset_index(drop=True)\n",
    "row4A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowA = row1A.append(row2A) #.append(row3A).append(row4A)\n",
    "rowA['Label'] = '1'\n",
    "rowA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowNa = row3A.append(row4A) #.append(row3A).append(row4A)\n",
    "rowNa['Label'] = '0'\n",
    "rowNa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fData = rowA.append(rowNa)\n",
    "fData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(fData['Label'].value_counts())[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "carrier_count = fData['Label'].value_counts()\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.barplot(carrier_count.index, carrier_count.values, alpha=0.9)\n",
    "plt.title('Frequency Distribution of Cat')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Cat', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fData.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "fData.dropna(inplace=True)\n",
    "fData.fillna(0)\n",
    "fData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fData.to_csv('Ganned-binary-merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newfData = shuffle(fData).reset_index(drop=True) \n",
    "newfData = newfData.iloc[0:150000,:]\n",
    "newfData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTrueData1 = data1.loc[data1['Label'] == 1].iloc[0:40000,:]\n",
    "newFalseData1 = data1.loc[data1['Label'] == 0]\n",
    "newData1 = pd.concat([newTrueData1, newFalseData1])\n",
    "newData1 = shuffle(newData1).reset_index(drop=True) \n",
    "#matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "carrier_count = newData1['Label'].value_counts()\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.barplot(carrier_count.index, carrier_count.values, alpha=0.9)\n",
    "plt.title('Frequency Distribution of Cat')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Cat', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = newfData['Label'].astype('int')\n",
    "X = newfData.drop(['Label'],axis=1) \n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# on this distribution. \n",
    "sc = MinMaxScaler()\n",
    "X_std =  sc.fit_transform(X)\n",
    "\n",
    "cov_matrix = np.cov(X_std.T)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "# Make a set of (eigenvalue, eigenvector) pairs:\n",
    "eig_pairs = [(eigenvalues[index], eigenvectors[:,index]) for index in range(len(eigenvalues))]\n",
    "# Sort the (eigenvalue, eigenvector) pairs from highest to lowest with respect to eigenvalue\n",
    "#eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "#print(eig_pairs)\n",
    "# Extract the descending ordered eigenvalues and eigenvectors\n",
    "eigvalues_sorted = [eig_pairs[index][0] for index in range(len(eigenvalues))]\n",
    "eigvectors_sorted = [eig_pairs[index][1] for index in range(len(eigenvalues))]\n",
    "# Let's confirm our sorting worked, print out eigenvalues\n",
    "#print('Eigenvalues in descending order: \\n%s' %eigvalues_sorted)\n",
    "\n",
    "tot = sum(eigenvalues)\n",
    "var_explained = [(i / tot) for i in sorted(eigenvalues, reverse=True)]  # an array of variance explained by each \n",
    "# eigen vector... there will be 18 entries as there are 18 eigen vectors)\n",
    "cum_var_exp = np.cumsum(var_explained)  # an array of cumulative variance. There will be 18 entries with 18 th entry \n",
    "# cumulative reaching almost 100%\n",
    "\n",
    "plt.bar(range(1,74), var_explained, alpha=0.5, align='center', label='individual explained variance')\n",
    "plt.step(range(1,74),cum_var_exp, where= 'mid', label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# feature selection\n",
    "def select_featuresMutual(X, y):\n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k=4)\n",
    "    X_new = fs.fit_transform(X, y)\n",
    "    return X_new\n",
    "\n",
    "# feature selection\n",
    "def select_featuresPCA(X, y, n_comp):\n",
    "    fs = PCA(n_components=n_comp)\n",
    "    X_new = fs.fit_transform(X, y)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,plot_confusion_matrix\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from statistics import mean \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "\n",
    "# This method will give the max value item in list\n",
    "def my_max_by_weight(sequence):\n",
    "    if not sequence:\n",
    "        raise ValueError('empty sequence')\n",
    "    maximum = sequence[0]\n",
    "    for item in sequence:\n",
    "        if item[0] > maximum[0]:\n",
    "            maximum = item\n",
    "    return maximum\n",
    "\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def DTAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    max_depths = np.linspace(1, 10, 10, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for max_depth in max_depths:\n",
    "        dt = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, max_depth])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = DecisionTreeClassifier(max_depth=my_max_by_weight(cValueList)[1], random_state=0)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def RFAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    max_depths = np.linspace(1, 10, 10, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for max_depth in max_depths:\n",
    "        dt = RandomForestClassifier(max_depth=max_depth, random_state=0)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, max_depth])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = RandomForestClassifier(max_depth=my_max_by_weight(cValueList)[1], random_state=0)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "def GBAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    max_depths = np.linspace(100, 120, 10, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for max_depth in max_depths:\n",
    "        dt = GradientBoostingClassifier(n_estimators=max_depth)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, max_depth])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = GradientBoostingClassifier(n_estimators=my_max_by_weight(cValueList)[1])\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "def NBAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    max_depths = np.logspace(0,-9, num=10)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for max_depth in max_depths:\n",
    "        dt = GaussianNB(var_smoothing=max_depth)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, max_depth])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = GaussianNB(var_smoothing=my_max_by_weight(cValueList)[1])\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def LogisticRegressionAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    cS = np.linspace(1, 10, 10, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for c in cS:\n",
    "        dt = LogisticRegression(C=c, random_state=0)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, c])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = LogisticRegression(C=my_max_by_weight(cValueList)[1], random_state=0)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "# This will give the classification results using KNeighbors  model\n",
    "def KNeighborsAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    n_neighbors = np.linspace(1, 10, 10, endpoint=True, dtype= int)\n",
    "    # object of train and testing arrays from TFIDF vectorizer\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with n_neighbors value for hyper parameter\n",
    "    for n_neighbor in n_neighbors:\n",
    "        dt = KNeighborsClassifier(n_neighbors=n_neighbor,metric='minkowski',p=2)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, n_neighbor])\n",
    "    # using n_neighbors that has most accuracy in the range in the list\n",
    "    dt = KNeighborsClassifier(n_neighbors=my_max_by_weight(cValueList)[1],metric='minkowski',p=2)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def AdaBoostClassifierAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    #max_depths = np.linspace(1, 10, 10, endpoint=True, dtype= int)\n",
    "    n_trees = [10, 50, 100, 500, 1000, 5000]\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for n in n_trees:\n",
    "        dt = AdaBoostClassifier(n_estimators=n,learning_rate=1)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, n])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = AdaBoostClassifier(n_estimators=my_max_by_weight(cValueList)[1])\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def XGBClassifierAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    max_depths = np.linspace(1, 10, 10, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for max_depth in max_depths:\n",
    "        dt = XGBClassifier(max_depth=max_depth, max_delta_step=0)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, max_depth])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = XGBClassifier(max_depth=my_max_by_weight(cValueList)[1], max_delta_step=0)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def SVCAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    cS = np.linspace(1, 10, 10, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for c in cS:\n",
    "        dt = SVC(C = c , kernel=\"rbf\",random_state=0)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, c])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = SVC(C=my_max_by_weight(cValueList)[1], kernel=\"rbf\",random_state=0)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "\n",
    "def SoftVotingAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    models = list()\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeRegressor()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('MLP', MLPClassifier()))\n",
    "    models.append(('DT', DecisionTreeClassifier()))\n",
    "    models.append(('RF', RandomForestClassifier()))\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('SVM', svm.SVC()))\n",
    "    models.append(('AdaBoost', AdaBoostClassifier()))\n",
    "    models.append(('GradientBoosting', GradientBoostingClassifier()))\n",
    "    models.append(('XGB', XGBClassifier()))\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for c in models:\n",
    "        dt = VotingClassifier(estimators=c, voting='soft')\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, c])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = VotingClassifier(estimators=my_max_by_weight(cValueList)[1], voting='soft')\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1y = data1['Label']\n",
    "data1X = data1.drop(['Label'],axis=1)\n",
    "\n",
    "X_s = select_featuresPCA(X, y, len(data1X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1y = newData1['Label']\n",
    "data1X = newData1.drop(['Label'],axis=1)\n",
    "\n",
    "X_s = select_featuresPCA(X, y, len(data1X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DT\",DTAccuracy(X_s, data1X, y, data1y))\n",
    "print(\"RF\", RFAccuracy(X_s, data1X, y, data1y))\n",
    "print(\"NB\", NBAccuracy(X_s, data1X, y, data1y))\n",
    "print(\"LR\", LogisticRegressionAccuracy(X_s, data1X, y, data1y))\n",
    "knn = KNeighborsAccuracy(X_s, data1X, y, data1y)\n",
    "print(\"KNN\", knn)\n",
    "svm = SVCAccuracy(X_s, data1X, y, data1y)\n",
    "print(\"SVM\", svm)\n",
    "ada = AdaBoostClassifierAccuracy(X_s, data1X, y, data1y)\n",
    "print(\"ADA\", ada)\n",
    "xgb = XGBClassifierAccuracy(X_s, data1X.values, y, data1y)\n",
    "print(\"XGB\", xgb)\n",
    "gb = GBAccuracy(X_s, data1X, y, data1y)\n",
    "print(\"gb\", gb)\n",
    "sv = SoftVotingAccuracy(X_s, data1X, y, data1y)\n",
    "print(\"gb\", sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2y = data2['Label']\n",
    "data2X = data2.drop(['Label'],axis=1)\n",
    "X2_s = select_featuresPCA(X, y, len(data2X.columns))\n",
    "\n",
    "\n",
    "dt2 = DTAccuracy(X2_s, data2X, y, data2y)\n",
    "print(\"DT\",dt2)\n",
    "rf2 = RFAccuracy(X2_s, data2X, y, data2y)\n",
    "print(\"RF\", rf2)\n",
    "nb2 = NBAccuracy(X2_s, data2X, y, data2y)\n",
    "print(\"RF\", nb2)\n",
    "lr2 = LogisticRegressionAccuracy(X2_s, data2X, y, data2y)\n",
    "print(\"LR\", lr2)\n",
    "knn2 = KNeighborsAccuracy(X2_s, data2X, y, data2y)\n",
    "print(\"KNN\", knn2)\n",
    "svm2 = SVCAccuracy(X2_s, data2X, y, data2y)\n",
    "print(\"SVM\", svm2)\n",
    "ada2 = AdaBoostClassifierAccuracy(X2_s, data2X, y, data2y)\n",
    "print(\"ADA\", ada2)\n",
    "xgb2 = XGBClassifierAccuracy(X2_s, data2X.values, y, data2y)\n",
    "print(\"XGB\", xgb2)\n",
    "gb2 = GBAccuracy(X2_s, data2X, y, data2y)\n",
    "print(\"XGB\", gb2)\n",
    "sv = SoftVotingAccuracy(X2_s, data2X, y, data2y)\n",
    "print(\"gb\", sv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
