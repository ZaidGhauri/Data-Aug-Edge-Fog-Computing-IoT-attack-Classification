{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "def readDataSets():\n",
    "    dataSetFridge = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/cat/IoT_Fridge.csv')\n",
    "    dataSetGarageDoor = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/cat/IoT_Garage_Door.csv')\n",
    "    dataSetGPS = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/cat/IoT_GPS_Tracker.csv')\n",
    "    dataSetModbus = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/cat/IoT_Modbus.csv')\n",
    "    dataSetMotionLight = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/cat/IoT_Motion_Light.csv')\n",
    "    dataSetThermostat = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/cat/IoT_Thermostat.csv')\n",
    "    dataSetWeahter = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/cat/IoT_Weather.csv')\n",
    "\n",
    "    dataSetFridge['temp_condition'] = dataSetFridge['temp_condition'].str.strip()\n",
    "    dataSetGarageDoor['door_state'] = dataSetGarageDoor['door_state'].str.strip()\n",
    "    dataSetMotionLight['light_status'] = dataSetMotionLight['light_status'].str.strip()\n",
    "    \n",
    "    dataSetFridge = cutSize(dataSetFridge)\n",
    "    dataSetGarageDoor = cutSize(dataSetGarageDoor)\n",
    "    dataSetGPS = cutSize(dataSetGPS)\n",
    "    dataSetModbus = cutSize(dataSetModbus)\n",
    "    dataSetMotionLight = cutSize(dataSetMotionLight)\n",
    "    dataSetThermostat = cutSize(dataSetThermostat)\n",
    "    dataSetWeahter = cutSize(dataSetWeahter)\n",
    "    \n",
    "    \n",
    "    dataSetRawLoad = pd.concat([dataSetFridge, dataSetGarageDoor, dataSetGPS, dataSetModbus, dataSetMotionLight, dataSetThermostat, dataSetWeahter])\n",
    "    #dataSetRawLoad = pd.concat([dataSetFridge, dataSetGarageDoor, dataSetGPS, dataSetModbus])\n",
    "    \n",
    "    print('dataSetRawLoad: ', dataSetRawLoad.shape)\n",
    "    return dataSetRawLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "categorical_features = ['type', 'door_state','sphone_signal', 'light_status','temp_condition']\n",
    "quantitative_features = ['FC1_Read_Input_Register','FC2_Read_Discrete_Value','FC3_Read_Holding_Register','FC4_Read_Coil','current_temperature',\n",
    "                       'fridge_temperature','humidity','latitude','FC4_Read_Coil','longitude',\n",
    "                      'motion_status','pressure','temperature','thermostat_status']\n",
    "#categorical_features = ['type', 'door_state','sphone_signal', 'temp_condition']\n",
    "#quantitative_features = ['FC1_Read_Input_Register','FC2_Read_Discrete_Value','FC3_Read_Holding_Register','FC4_Read_Coil',\n",
    "#                        'fridge_temperature',\n",
    "#                         #'humidity' ,\n",
    "#                         'latitude','FC4_Read_Coil','longitude',\n",
    "#                        #'pressure',\n",
    "#                        # 'temperature'\n",
    "#                        ]\n",
    "features = categorical_features + quantitative_features\n",
    "\n",
    "def datapreprocessingShuffle(data):\n",
    "               \n",
    "    # Feature scaling\n",
    "    for i in quantitative_features :\n",
    "            #scaler = StandardScaler()\n",
    "            #data[i] = scaler.fit_transform(data[[i]])\n",
    "            data[i] = data[i]\n",
    "            \n",
    "    # Encoding categorical features    \n",
    "    for i in categorical_features : \n",
    "        labelencoder=LabelEncoder()\n",
    "        data[i]=labelencoder.fit_transform(data[i])   \n",
    "    \n",
    "    data = shuffle(data).reset_index(drop=True) \n",
    "    \n",
    "    Y = data.loc[:,'type']\n",
    "    X = data.drop(['type'],axis=1) \n",
    "    \n",
    "    return(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def funcTransform(d):\n",
    "    d1 = d.iloc[:,:-1]\n",
    "    \n",
    "    dN =pd.DataFrame()\n",
    "    label=LabelEncoder()\n",
    "    for c in  d1.columns:\n",
    "        if(d[c].dtype=='object'):\n",
    "            dN[c]=label.fit_transform(d1[c])\n",
    "        else:\n",
    "            dN[c]=d1[c]\n",
    "            \n",
    "    c = dN.columns\n",
    "    ff_arr = preprocessing.normalize(dN)\n",
    "    df1 = pd.DataFrame(data=ff_arr, columns=c)\n",
    "    df1['label'] = d['type']\n",
    "    return df1\n",
    "\n",
    "def funcTransform1(d):\n",
    "    dN =pd.DataFrame()\n",
    "    label=LabelEncoder()\n",
    "    for c in  d.columns:\n",
    "        if(d[c].dtype=='object'):\n",
    "            dN[c]=label.fit_transform(d[c])\n",
    "        else:\n",
    "            dN[c]=d[c]\n",
    "            \n",
    "    return dN\n",
    "\n",
    "def cutSize(df):\n",
    "    newdf1 = df[df['type'] == 'password'].iloc[0:30000,:]\n",
    "    newdf2 = df[df['type'] == 'injection'].iloc[0:30000,:]\n",
    "    #newdf3 = df[df['type'] == 'backdoor'].iloc[0:5000,:]\n",
    "    #newdf4 = df[df['type'] == 'ddos'].iloc[0:15000,:]\n",
    "    #newdf5 = df[df['type'] == 'ransomware'].iloc[0:15000,:]\n",
    "    #newdf6 = df[df['type'] == 'xss'].iloc[0:15000,:]\n",
    "    #newdf7 = df[df['type'] == 'scanning'].iloc[0:15000,:]\n",
    "    n = pd.concat([newdf1, newdf2]).sample(frac=1).reset_index(drop=True)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = readDataSets()\n",
    "df1 = df.copy()\n",
    "df2 = df.copy()\n",
    "dfm = pd.concat([df, df1, df2]).sample(frac=1).reset_index(drop=True)\n",
    "print(dfm.shape)\n",
    "dfo = cutSize(dfm)\n",
    "# Pre-processing datset\n",
    "datacopy = dfo.copy()\n",
    "X, y = datapreprocessingShuffle(datacopy) \n",
    "\n",
    "X = X.fillna(X.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dfo['type'].value_counts())[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,plot_confusion_matrix\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from statistics import mean \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "import time\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "# This method will give the max value item in list\n",
    "def my_max_by_weight(sequence):\n",
    "    if not sequence:\n",
    "        raise ValueError('empty sequence')\n",
    "    maximum = sequence[0]\n",
    "    for item in sequence:\n",
    "        if item[0] > maximum[0]:\n",
    "            maximum = item\n",
    "    return maximum\n",
    "\n",
    "\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def LogisticRegressionAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    cS = np.linspace(1, 100, 100, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for c in cS:\n",
    "        dt = LogisticRegression(C=c, random_state=0)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, c])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = LogisticRegression(C=my_max_by_weight(cValueList)[1], random_state=0)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred, average='weighted')\n",
    "    recall = recall_score(y_test,y_pred, average='weighted')\n",
    "    score = f1_score(y_test, y_pred, average='weighted')\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "\n",
    "# This will give the classification results using KNeighbors  model\n",
    "def KNeighborsAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    n_neighbors = np.linspace(1, 100, 50, endpoint=True, dtype= int)\n",
    "    # object of train and testing arrays from TFIDF vectorizer\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with n_neighbors value for hyper parameter\n",
    "    for n_neighbor in n_neighbors:\n",
    "        dt = KNeighborsClassifier(n_neighbors=n_neighbor,metric='minkowski',p=2)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, n_neighbor])\n",
    "    # using n_neighbors that has most accuracy in the range in the list\n",
    "    dt = KNeighborsClassifier(n_neighbors=my_max_by_weight(cValueList)[1],metric='minkowski',p=2)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred, average='weighted')\n",
    "    recall = recall_score(y_test,y_pred, average='weighted')\n",
    "    score = f1_score(y_test, y_pred, average='weighted')\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def DTAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    max_depths = np.linspace(1, 100, 100, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for max_depth in max_depths:\n",
    "        dt = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, max_depth])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = DecisionTreeClassifier(max_depth=my_max_by_weight(cValueList)[1], random_state=0)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred, average='weighted')\n",
    "    recall = recall_score(y_test,y_pred, average='weighted')\n",
    "    score = f1_score(y_test, y_pred, average='weighted')\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def RFAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    max_depths = np.linspace(1, 100, 100, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for max_depth in max_depths:\n",
    "        dt = RandomForestClassifier(max_depth=max_depth, random_state=0)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, max_depth])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = RandomForestClassifier(max_depth=my_max_by_weight(cValueList)[1], random_state=0)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred, average='weighted')\n",
    "    recall = recall_score(y_test,y_pred, average='weighted')\n",
    "    score = f1_score(y_test, y_pred, average='weighted')\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "def GBAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    max_depths = np.linspace(100, 120, 10, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for max_depth in max_depths:\n",
    "        dt = GradientBoostingClassifier(n_estimators=max_depth)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, max_depth])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = GradientBoostingClassifier(n_estimators=my_max_by_weight(cValueList)[1])\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred, average='weighted')\n",
    "    recall = recall_score(y_test,y_pred, average='weighted')\n",
    "    score = f1_score(y_test, y_pred, average='weighted')\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "def NBAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    max_depths = np.logspace(0,-9, num=10)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for max_depth in max_depths:\n",
    "        dt = GaussianNB(var_smoothing=max_depth)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, max_depth])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = GaussianNB(var_smoothing=my_max_by_weight(cValueList)[1])\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred, average='weighted')\n",
    "    recall = recall_score(y_test,y_pred, average='weighted')\n",
    "    score = f1_score(y_test, y_pred, average='weighted')\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def AdaBoostClassifierAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    #max_depths = np.linspace(1, 10, 10, endpoint=True, dtype= int)\n",
    "    n_trees = [10, 50, 100, 500, 1000, 5000]\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for n in n_trees:\n",
    "        dt = AdaBoostClassifier(n_estimators=n,learning_rate=1)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, n])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = AdaBoostClassifier(n_estimators=my_max_by_weight(cValueList)[1])\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred, average='weighted')\n",
    "    recall = recall_score(y_test,y_pred, average='weighted')\n",
    "    score = f1_score(y_test, y_pred, average='weighted')\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def XGBClassifierAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    max_depths =np.linspace(1, 100, 100, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for max_depth in max_depths:\n",
    "        dt = XGBClassifier(max_depth=max_depth, max_delta_step=0)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, max_depth])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = XGBClassifier(max_depth=my_max_by_weight(cValueList)[1], max_delta_step=0)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred, average='weighted')\n",
    "    recall = recall_score(y_test,y_pred, average='weighted')\n",
    "    score = f1_score(y_test, y_pred, average='weighted')\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def SVCAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    cS = np.linspace(1, 100, 100, endpoint=True, dtype= int)\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for c in cS:\n",
    "        dt = SVC(C = c , kernel=\"rbf\",random_state=0)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, c])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = SVC(C=my_max_by_weight(cValueList)[1], kernel=\"rbf\",random_state=0)\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred, average='weighted')\n",
    "    recall = recall_score(y_test,y_pred, average='weighted')\n",
    "    score = f1_score(y_test, y_pred, average='weighted')\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj\n",
    "\n",
    "# This will give the classification results using Logistic Regression model\n",
    "def LDAAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    cS = ['svd', 'lsqr', 'eigen']\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for c in cS:\n",
    "        dt = LinearDiscriminantAnalysis(solver = c)\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, c])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = LinearDiscriminantAnalysis(solver=my_max_by_weight(cValueList)[1])\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred, average='weighted')\n",
    "    recall = recall_score(y_test,y_pred, average='weighted')\n",
    "    score = f1_score(y_test, y_pred, average='weighted')\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy}\n",
    "    return resultObj\n",
    "\n",
    "def SoftVotingAccuracy(X_train, X_test, y_train, y_test):\n",
    "    # create a variable for the range from 1 to 100\n",
    "    models = list()\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeRegressor()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('MLP', MLPClassifier()))\n",
    "    models.append(('DT', DecisionTreeClassifier()))\n",
    "    models.append(('RF', RandomForestClassifier()))\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('SVM', svm.SVC()))\n",
    "    models.append(('AdaBoost', AdaBoostClassifier()))\n",
    "    models.append(('GradientBoosting', GradientBoostingClassifier()))\n",
    "    models.append(('XGB', XGBClassifier()))\n",
    "    # properties initialization\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    cValueList = []\n",
    "    # loop that applied range on the model one by one and return a list of accuracies with C value for hyper parameter\n",
    "    for c in models:\n",
    "        dt = VotingClassifier(estimators=c, voting='soft')\n",
    "        dt.fit(X_train,y_train)\n",
    "        pred_result = dt.predict(X_test)\n",
    "        score2 = dt.score(X_test, y_test)\n",
    "        cValueList.append([score2, c])\n",
    "    # using C that has most accuracy in the range in the list\n",
    "    dt = VotingClassifier(estimators=my_max_by_weight(cValueList)[1], voting='soft')\n",
    "    # fit the train and test\n",
    "    dt.fit(X_train, y_train)\n",
    "    # plot confusion matrix \n",
    "    plot_confusion_matrix(dt, X_test, y_test)\n",
    "    plt.show()\n",
    "    y_pred = dt.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    resultObj = {\"precision\":precision,\"recall\":recall,\"f1score\":score,\"accuracy\":accuracy,\"cm\":cm}\n",
    "    return resultObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection  import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "# We use a utility to generate artificial classification data.\n",
    "X = X.values\n",
    "y = y.values\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "sss.get_n_splits(X, y)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    lr = LogisticRegressionAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"LR\", lr)\n",
    "    knn = KNeighborsAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"KNN\", knn)\n",
    "    dt = DTAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"DT\", dt)\n",
    "    rf = RFAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"RF\", rf)\n",
    "    gb = GBAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"GB\", gb)\n",
    "    nb = NBAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"NB\", nb)\n",
    "    ada = AdaBoostClassifierAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"ADA\", ada)\n",
    "    xgb = XGBClassifierAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"XGB\", xgb)\n",
    "    svm = SVCAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"SVM\", svm)\n",
    "    lda = LDAAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"LDA\", lda)\n",
    "    sv = SoftVotingAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"gb\", sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude scanning\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection  import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "# We use a utility to generate artificial classification data.\n",
    "X = X.values\n",
    "y = y.values\n",
    "sss = StratifiedShuffleSplit(n_splits=3, test_size=0.25, random_state=0)\n",
    "sss.get_n_splits(X, y)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    lr = LogisticRegressionAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"LR\", lr)\n",
    "    knn = KNeighborsAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"KNN\", knn)\n",
    "    dt = DTAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"DT\", dt)\n",
    "    rf = RFAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"RF\", rf)\n",
    "    gb = GBAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"GB\", gb)\n",
    "    nb = NBAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"NB\", nb)\n",
    "    ada = AdaBoostClassifierAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"ADA\", ada)\n",
    "    xgb = XGBClassifierAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"XGB\", xgb)\n",
    "    svm = SVCAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"SVM\", svm)\n",
    "    lda = LDAAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"LDA\", lda)\n",
    "      sv = SoftVotingAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"gb\", sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude scanning\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection  import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "# We use a utility to generate artificial classification data.\n",
    "X = X.values\n",
    "y = y.values\n",
    "sss = StratifiedShuffleSplit(n_splits=4, test_size=0.2, random_state=0)\n",
    "sss.get_n_splits(X, y)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    lr = LogisticRegressionAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"LR\", lr)\n",
    "    knn = KNeighborsAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"KNN\", knn)\n",
    "    dt = DTAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"DT\", dt)\n",
    "    rf = RFAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"RF\", rf)\n",
    "    gb = GBAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"GB\", gb)\n",
    "    nb = NBAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"NB\", nb)\n",
    "    ada = AdaBoostClassifierAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"ADA\", ada)\n",
    "    xgb = XGBClassifierAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"XGB\", xgb)\n",
    "    svm = SVCAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"SVM\", svm)\n",
    "    lda = LDAAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"LDA\", lda)\n",
    "      sv = SoftVotingAccuracy(X_train, X_test, y_train, y_test)\n",
    "    print(\"gb\", sv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
