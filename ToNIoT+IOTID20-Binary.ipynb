{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "ioTInTData = pd.read_csv(\"data/IoTID20/IoT_Network_Intrusion_Dataset.csv\" , low_memory=False)\n",
    "ioTInTData = ioTInTData[ioTInTData['Cat'] != 'Mirai']\n",
    "ioTInTData = ioTInTData.iloc[:,:80]\n",
    "print('ioTInTData', ioTInTData.shape)\n",
    "\n",
    "#ioTInTData = ioTInTData.fillna(np.nan, inplace=True)\n",
    "ioTInTData.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "ioTInTData.dropna(inplace=True)\n",
    "ioTInTData.fillna(0)\n",
    "ioTInTData.shape\n",
    "\n",
    "def readDataSets():\n",
    "    dataSetFridge = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Fridge.csv')\n",
    "    dataSetGarageDoor = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Garage_Door.csv')\n",
    "    dataSetGPS = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_GPS_Tracker.csv')\n",
    "    dataSetModbus = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Modbus.csv')\n",
    "    dataSetMotionLight = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Motion_Light.csv')\n",
    "    dataSetThermostat = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Thermostat.csv')\n",
    "    dataSetWeahter = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/binary/IoT_Weather.csv')\n",
    "\n",
    "    dataSetFridge['temp_condition'] = dataSetFridge['temp_condition'].str.strip()\n",
    "    dataSetGarageDoor['door_state'] = dataSetGarageDoor['door_state'].str.strip()\n",
    "    dataSetMotionLight['light_status'] = dataSetMotionLight['light_status'].str.strip()\n",
    "    dataSetRawLoad = pd.concat([dataSetFridge, dataSetGarageDoor, dataSetGPS, dataSetModbus, dataSetMotionLight, dataSetThermostat, dataSetWeahter])\n",
    "    return dataSetRawLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = readDataSets()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "categorical_features = ['door_state','sphone_signal', 'light_status','temp_condition']\n",
    "quantitative_features = ['FC1_Read_Input_Register','FC2_Read_Discrete_Value','FC3_Read_Holding_Register','FC4_Read_Coil','current_temperature',\n",
    "                        'fridge_temperature','humidity','latitude','FC4_Read_Coil','longitude',\n",
    "                        'motion_status','pressure','temperature','thermostat_status']\n",
    "features = categorical_features + quantitative_features\n",
    "\n",
    "\n",
    "def datapreprocessingShuffle(data):\n",
    "    scaler = StandardScaler()            \n",
    "    # Feature scaling\n",
    "    for i in data.columns:\n",
    "        if(data[i].name !='Label'):\n",
    "            data[i] = scaler.fit_transform(data[[i]])\n",
    "        else:\n",
    "            data[i]=data[i]\n",
    "            \n",
    "    data = shuffle(data).reset_index(drop=True) \n",
    "    return data\n",
    "\n",
    "\n",
    "def datapreprocessingShuffle2(data):\n",
    "               \n",
    "    # Feature scaling\n",
    "    for i in quantitative_features :\n",
    "            scaler = StandardScaler()\n",
    "            data[i] = scaler.fit_transform(data[[i]])\n",
    "            \n",
    "    # Encoding categorical features    \n",
    "    for i in categorical_features :\n",
    "        labelencoder=LabelEncoder()\n",
    "        data[i]=labelencoder.fit_transform(data[i])   \n",
    "    \n",
    "    data = shuffle(data).reset_index(drop=True) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing datset\n",
    "datacopy1 = ioTInTData.copy()\n",
    "datacopy2 = df.copy()\n",
    "datacopy1['Label'] = datacopy1.iloc[:,79:80].replace('Normal', '0').replace('Anomaly', '1').astype('int')\n",
    "data1 = datapreprocessingShuffle(datacopy1)\n",
    "\n",
    "data2 = datacopy2.rename(columns={\"label\": \"Label\"})\n",
    "data2 = datapreprocessingShuffle2(data2) \n",
    "# fill na with mean for ToN_IoT dataset\n",
    "data2 = data2.fillna(data2.mean())\n",
    "print(data1.shape)\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of columns\n",
    "cols = list(data2)\n",
    "# move the column to head of list using index, pop and insert\n",
    "cols.insert(18, cols.pop(cols.index('Label')))\n",
    "\n",
    "# use ix to reorder\n",
    "data2 = data2.ix[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for false labels\n",
    "#data2 = data2.loc[data2['Label'] == 0] #no attack\n",
    "\n",
    "# for false labels\n",
    "data2 = data2.loc[data2['Label'] == 1] #attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GAN_171103\n",
    "import importlib\n",
    "importlib.reload(GAN_171103) # For reloading after making changes\n",
    "from GAN_171103 import *\n",
    "\n",
    "\n",
    " # 32 # needs to be ~data_dim\n",
    "base_n_count = 128 # 128\n",
    "\n",
    "nb_steps = 500 + 1 # 50000 # Add one for logging of the last interval\n",
    "batch_size = 128 # 64\n",
    "\n",
    "k_d = 1  # number of critic network updates per adversarial training step\n",
    "k_g = 1  # number of generator network updates per adversarial training step\n",
    "critic_pre_train_steps = 100 # 100  # number of steps to pre-train the critic before starting adversarial training\n",
    "log_interval = 100 # 100  # interval (in steps) at which to log loss summaries and save plots of image samples to disc\n",
    "learning_rate = 5e-4 # 5e-5\n",
    "data_dir = 'cache/'\n",
    "generator_model_path, discriminator_model_path, loss_pickle_path = None, None, None\n",
    "\n",
    "# show = False\n",
    "show = True \n",
    "\n",
    "# train = create_toy_spiral_df(1000)\n",
    "# train = create_toy_df(n=1000,n_dim=2,n_classes=4,seed=0)\n",
    "train = data2.copy().reset_index(drop=True) # fraud only with labels from classification\n",
    "\n",
    "\n",
    "# train = pd.get_dummies(train, columns=['Class'], prefix='Class', drop_first=True)\n",
    "label_cols = [ i for i in train.columns if 'Label' in i ]\n",
    "data_cols = [ i for i in train.columns if i not in label_cols ]\n",
    "train[ data_cols ] = train[ data_cols ] / 10 # scale to random noise size, one less thing to learn\n",
    "train_no_label = train[ data_cols ]\n",
    "\n",
    "\n",
    "rand_dim = len(data_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training the vanilla GAN and CGAN architectures\n",
    "\n",
    "k_d = 1  # number of critic network updates per adversarial training step\n",
    "learning_rate = 5e-4 # 5e-5\n",
    "arguments = [rand_dim, nb_steps, batch_size, \n",
    "             k_d, k_g, critic_pre_train_steps, log_interval, learning_rate, base_n_count,\n",
    "            data_dir, generator_model_path, discriminator_model_path, loss_pickle_path, show ]\n",
    "\n",
    "adversarial_training_GAN(arguments, train_no_label, data_cols ) # GAN\n",
    "#adversarial_training_GAN(arguments, train, data_cols=data_cols, label_cols=label_cols ) # CGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 17\n",
    "\n",
    "train = data2.copy().reset_index(drop=True) # fraud only with labels from classification\n",
    "\n",
    "# train = pd.get_dummies(train, columns=['Class'], prefix='Class', drop_first=True)\n",
    "label_cols = [ train.columns[-1]  ]\n",
    "data_cols = [ i for i in train.columns if i not in label_cols ]\n",
    "#train[ data_cols ] = train[ data_cols ] / 10 # scale to random noise size, one less thing to learn\n",
    "train_no_label = train[ data_cols ]\n",
    "\n",
    "data_dim = len(data_cols)\n",
    "label_dim = len(label_cols)\n",
    "#if label_dim > 0: with_class = True\n",
    "np.random.seed(seed)\n",
    "\n",
    "# define network models\n",
    "\n",
    "generator_model, discriminator_model, combined_model = define_models_GAN(rand_dim, data_dim, base_n_count)\n",
    "generator_model.load_weights('cache/GAN_generator_model_weights_step_500.h5')\n",
    "\n",
    "# Now generate some new data\n",
    "test_size = len(train) # Equal to all of the fraud cases\n",
    "\n",
    "x = get_data_batch(train_no_label, test_size, seed=3)\n",
    "z = np.random.normal(size=(test_size, rand_dim))\n",
    "g_z = generator_model.predict(z)\n",
    "    \n",
    "\n",
    "# =============================================================================\n",
    "# check data\n",
    "# =============================================================================\n",
    "df2=pd.DataFrame(np.rint(np.abs(g_z)),columns=df.columns[:-1])\n",
    "df2=df2.astype(int)\n",
    "\n",
    "\n",
    "def define_models_GAN(rand_dim, data_dim, base_n_count, type=None):\n",
    "    generator_input_tensor = layers.Input(shape=(rand_dim, ))\n",
    "    generated_image_tensor = generator_network(generator_input_tensor, data_dim, base_n_count)\n",
    "\n",
    "    generated_or_real_image_tensor = layers.Input(shape=(data_dim,))\n",
    "    \n",
    "    if type == 'Wasserstein':\n",
    "        discriminator_output = critic_network(generated_or_real_image_tensor, data_dim, base_n_count)\n",
    "    else:\n",
    "        discriminator_output = discriminator_network(generated_or_real_image_tensor, data_dim, base_n_count)\n",
    "\n",
    "    generator_model = models.Model(inputs=[generator_input_tensor], outputs=[generated_image_tensor], name='generator')\n",
    "    discriminator_model = models.Model(inputs=[generated_or_real_image_tensor],\n",
    "                                       outputs=[discriminator_output],\n",
    "                                       name='discriminator')\n",
    "\n",
    "    combined_output = discriminator_model(generator_model(generator_input_tensor))\n",
    "    combined_model = models.Model(inputs=[generator_input_tensor], outputs=[combined_output], name='combined')\n",
    "    \n",
    "    return generator_model, discriminator_model, combined_model\n",
    "\n",
    "def generate_data_gan(n_samples,Xdf,path_model,rand_dim=32):\n",
    "    generator_model, discriminator_model, combined_model= define_models_GAN(rand_dim,data_dim=len(Xdf.columns),base_n_count=128)\n",
    "    generator_model.load_weights(path_model)\n",
    "    z = np.random.normal(size=(n_samples, rand_dim))\n",
    "    g_z = generator_model.predict(z)\n",
    "    \n",
    "    newdf = pd.DataFrame()\n",
    "    for i, col in enumerate(Xdf.columns):\n",
    "\n",
    "        if Xdf[col].dtype == 'int32' or Xdf[col].dtype == 'int64':\n",
    "            newdf[col] = np.rint(np.abs(g_z[:,i])).astype(int)    \n",
    "        elif Xdf[col].dtype == 'float_':\n",
    "            newdf[col] = np.abs(g_z[:,i]) \n",
    "    return newdf\n",
    "\n",
    "\n",
    "#rand_dim = 32 \n",
    "#base_n_count = 128\n",
    "#data_dim = len(data_cols) \n",
    "#'nursery','mushroom'\n",
    "\n",
    "#data11 = 119698\n",
    "\n",
    "#data12-FALSE = 62% of data1 (0.62 * 119698)= 74213 Label = 0\n",
    "#data12-TRUE  = 38% of data1 (0.38 * 119698)= 45485 Label = 1\n",
    "# data2 False\n",
    "\n",
    "\n",
    "\n",
    "n_samples=45485\n",
    "\n",
    "# data2 True\n",
    "#n_samples=45485\n",
    "\n",
    "Xdf=data2.iloc[:,:-1]    \n",
    "path_model='cache/GAN_generator_model_weights_step_500.h5'\n",
    "\n",
    "#data12False=generate_data_gan(n_samples,Xdf,path_model,rand_dim=len(data_cols))  \n",
    "#data12False.shape\n",
    "\n",
    "data12True=generate_data_gan(n_samples,Xdf,path_model,rand_dim=len(data_cols))  \n",
    "data12True.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data12False['Label'] = 0\n",
    "print(data12False.shape)\n",
    "data12True['Label'] = 1\n",
    "print(data12True.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gen_data2 = pd.concat([data12True, data12False])\n",
    "Gen_data2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for false labels\n",
    "#data2 = data1.loc[data1['Label'] == 0] #no attack\n",
    "\n",
    "# for false labels\n",
    "data2 = data1.loc[data1['Label'] == 1] #attack\n",
    "\n",
    "import GAN_171103\n",
    "import importlib\n",
    "importlib.reload(GAN_171103) # For reloading after making changes\n",
    "from GAN_171103 import *\n",
    "\n",
    "\n",
    " # 32 # needs to be ~data_dim\n",
    "base_n_count = 128 # 128\n",
    "\n",
    "nb_steps = 500 + 1 # 50000 # Add one for logging of the last interval\n",
    "batch_size = 128 # 64\n",
    "\n",
    "k_d = 1  # number of critic network updates per adversarial training step\n",
    "k_g = 1  # number of generator network updates per adversarial training step\n",
    "critic_pre_train_steps = 100 # 100  # number of steps to pre-train the critic before starting adversarial training\n",
    "log_interval = 100 # 100  # interval (in steps) at which to log loss summaries and save plots of image samples to disc\n",
    "learning_rate = 5e-4 # 5e-5\n",
    "data_dir = 'cache/'\n",
    "generator_model_path, discriminator_model_path, loss_pickle_path = None, None, None\n",
    "\n",
    "# show = False\n",
    "show = True \n",
    "\n",
    "# train = create_toy_spiral_df(1000)\n",
    "# train = create_toy_df(n=1000,n_dim=2,n_classes=4,seed=0)\n",
    "train = data2.copy().reset_index(drop=True) # fraud only with labels from classification\n",
    "\n",
    "\n",
    "# train = pd.get_dummies(train, columns=['Class'], prefix='Class', drop_first=True)\n",
    "label_cols = [ i for i in train.columns if 'Label' in i ]\n",
    "data_cols = [ i for i in train.columns if i not in label_cols ]\n",
    "train[ data_cols ] = train[ data_cols ] / 10 # scale to random noise size, one less thing to learn\n",
    "train_no_label = train[ data_cols ]\n",
    "\n",
    "\n",
    "rand_dim = len(data_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training the vanilla GAN and CGAN architectures\n",
    "\n",
    "k_d = 1  # number of critic network updates per adversarial training step\n",
    "learning_rate = 5e-4 # 5e-5\n",
    "arguments = [rand_dim, nb_steps, batch_size, \n",
    "             k_d, k_g, critic_pre_train_steps, log_interval, learning_rate, base_n_count,\n",
    "            data_dir, generator_model_path, discriminator_model_path, loss_pickle_path, show ]\n",
    "\n",
    "adversarial_training_GAN(arguments, train_no_label, data_cols ) # GAN\n",
    "#adversarial_training_GAN(arguments, train, data_cols=data_cols, label_cols=label_cols ) # CGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 17\n",
    "\n",
    "train = data2.copy().reset_index(drop=True) # fraud only with labels from classification\n",
    "\n",
    "# train = pd.get_dummies(train, columns=['Class'], prefix='Class', drop_first=True)\n",
    "label_cols = [ train.columns[-1]  ]\n",
    "data_cols = [ i for i in train.columns if i not in label_cols ]\n",
    "#train[ data_cols ] = train[ data_cols ] / 10 # scale to random noise size, one less thing to learn\n",
    "train_no_label = train[ data_cols ]\n",
    "\n",
    "data_dim = len(data_cols)\n",
    "label_dim = len(label_cols)\n",
    "#if label_dim > 0: with_class = True\n",
    "np.random.seed(seed)\n",
    "\n",
    "# define network models\n",
    "\n",
    "generator_model, discriminator_model, combined_model = define_models_GAN(rand_dim, data_dim, base_n_count)\n",
    "generator_model.load_weights('cache/GAN_generator_model_weights_step_500.h5')\n",
    "\n",
    "\n",
    "# Now generate some new data\n",
    "test_size = len(train) # Equal to all of the fraud cases\n",
    "\n",
    "x = get_data_batch(train_no_label, test_size, seed=3)\n",
    "z = np.random.normal(size=(test_size, rand_dim))\n",
    "g_z = generator_model.predict(z)\n",
    "\n",
    " \n",
    "\n",
    "# =============================================================================\n",
    "# check data\n",
    "# =============================================================================\n",
    "df2=pd.DataFrame(np.rint(np.abs(g_z)),columns=data2.columns[:-1])\n",
    "df2=df2.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "def define_models_GAN(rand_dim, data_dim, base_n_count, type=None):\n",
    "    generator_input_tensor = layers.Input(shape=(rand_dim, ))\n",
    "    generated_image_tensor = generator_network(generator_input_tensor, data_dim, base_n_count)\n",
    "\n",
    "    generated_or_real_image_tensor = layers.Input(shape=(data_dim,))\n",
    "    \n",
    "    if type == 'Wasserstein':\n",
    "        discriminator_output = critic_network(generated_or_real_image_tensor, data_dim, base_n_count)\n",
    "    else:\n",
    "        discriminator_output = discriminator_network(generated_or_real_image_tensor, data_dim, base_n_count)\n",
    "\n",
    "    generator_model = models.Model(inputs=[generator_input_tensor], outputs=[generated_image_tensor], name='generator')\n",
    "    discriminator_model = models.Model(inputs=[generated_or_real_image_tensor],\n",
    "                                       outputs=[discriminator_output],\n",
    "                                       name='discriminator')\n",
    "\n",
    "    combined_output = discriminator_model(generator_model(generator_input_tensor))\n",
    "    combined_model = models.Model(inputs=[generator_input_tensor], outputs=[combined_output], name='combined')\n",
    "    \n",
    "    return generator_model, discriminator_model, combined_model\n",
    "\n",
    "def generate_data_gan(n_samples,Xdf,path_model,rand_dim=32):\n",
    "    generator_model, discriminator_model, combined_model= define_models_GAN(rand_dim,data_dim=len(Xdf.columns),base_n_count=128)\n",
    "    generator_model.load_weights(path_model)\n",
    "    z = np.random.normal(size=(n_samples, rand_dim))\n",
    "    g_z = generator_model.predict(z)\n",
    "    \n",
    "    newdf = pd.DataFrame()\n",
    "    for i, col in enumerate(Xdf.columns):\n",
    "\n",
    "        if Xdf[col].dtype == 'int32' or Xdf[col].dtype == 'int64':\n",
    "            newdf[col] = np.rint(np.abs(g_z[:,i])).astype(int)    \n",
    "        elif Xdf[col].dtype == 'float_':\n",
    "            newdf[col] = np.abs(g_z[:,i]) \n",
    "    return newdf\n",
    "\n",
    "\n",
    "\n",
    "#data22 = 401119\n",
    "\n",
    "#data21-FALSE = 23% of data1 (0.23 * 401119)= 92257 Label = 0\n",
    "#data21-TRUE  = 77% of data1 (0.77 * 401119)= 308862 Label = 1\n",
    "\n",
    "n_samples=308862\n",
    "\n",
    "# data2 True\n",
    "#n_samples=45485\n",
    "\n",
    "Xdf=data2.iloc[:,:-1]    \n",
    "path_model='cache/GAN_generator_model_weights_step_500.h5'\n",
    "\n",
    "#data21False=generate_data_gan(n_samples,Xdf,path_model,rand_dim=len(data_cols))  \n",
    "#data21False.shape\n",
    "\n",
    "data21True=generate_data_gan(n_samples,Xdf,path_model,rand_dim=len(data_cols))  \n",
    "data21True.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data22False['Label'] = 0\n",
    "print(data22False.shape)\n",
    "data22True['Label'] = 1\n",
    "print(data22True.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gen_data1 = pd.concat([data22True, data22False])\n",
    "Gen_data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat data22 with generated data12\n",
    "\n",
    "#              Col1           |     Col2\n",
    "#  Row1    data11 (IoTID20)  |  data12 (GAN)\n",
    "#         _____________________________________\n",
    "#          \n",
    "#  Row2    data21 (GAN)     |  data22 (TonIoT)\n",
    "#\n",
    "# data22 = TonIoT\n",
    "# data12 = Gen_data2\n",
    "# Row1(520817) = data11(119698) + data12(401119)\n",
    "Row1Data = pd.concat([data1, Gen_data2])\n",
    "Row1Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat data22 with generated data12\n",
    "\n",
    "#    Col1           |     Col2\n",
    "# data11 (IoTID20)  |  data12 (GAN)\n",
    "# _____________________________________\n",
    "#          \n",
    "# data21 (GAN)     |  data22 (TonIoT)\n",
    "#\n",
    "# data11 = IoTID20\n",
    "# data21 = Gen_data1\n",
    "# Col1(520817) = data11(119698) + data21(401119)\n",
    "row2Data = pd.concat([Gen_data1, data2])\n",
    "row2Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tData1 = data1.loc[data1['Label'] == 1] \n",
    "tData1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalData = pd.concat([col1Data, col2Data])\n",
    "totalData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na with mean for ToN_IoT dataset\n",
    "fData = pd.concat([data1, data2])\n",
    "fData = fData.fillna(fData.mean())\n",
    "fData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(fData['Label'].value_counts())[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "carrier_count = fData['Label'].value_counts()\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.barplot(carrier_count.index, carrier_count.values, alpha=0.9)\n",
    "plt.title('Frequency Distribution of Cat')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Cat', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fData['Label']\n",
    "X = fData.drop(['Label'],axis=1) \n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# on this distribution. \n",
    "sc = MinMaxScaler()\n",
    "X_std =  sc.fit_transform(X)\n",
    "\n",
    "cov_matrix = np.cov(X_std.T)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "# Make a set of (eigenvalue, eigenvector) pairs:\n",
    "eig_pairs = [(eigenvalues[index], eigenvectors[:,index]) for index in range(len(eigenvalues))]\n",
    "# Sort the (eigenvalue, eigenvector) pairs from highest to lowest with respect to eigenvalue\n",
    "#eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "#print(eig_pairs)\n",
    "# Extract the descending ordered eigenvalues and eigenvectors\n",
    "eigvalues_sorted = [eig_pairs[index][0] for index in range(len(eigenvalues))]\n",
    "eigvectors_sorted = [eig_pairs[index][1] for index in range(len(eigenvalues))]\n",
    "# Let's confirm our sorting worked, print out eigenvalues\n",
    "#print('Eigenvalues in descending order: \\n%s' %eigvalues_sorted)\n",
    "\n",
    "tot = sum(eigenvalues)\n",
    "var_explained = [(i / tot) for i in sorted(eigenvalues, reverse=True)]  # an array of variance explained by each \n",
    "# eigen vector... there will be 18 entries as there are 18 eigen vectors)\n",
    "cum_var_exp = np.cumsum(var_explained)  # an array of cumulative variance. There will be 18 entries with 18 th entry \n",
    "# cumulative reaching almost 100%\n",
    "\n",
    "plt.bar(range(1,97), var_explained, alpha=0.5, align='center', label='individual explained variance')\n",
    "plt.step(range(1,97),cum_var_exp, where= 'mid', label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# feature selection\n",
    "def select_featuresMutual(X, y):\n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k=4)\n",
    "    X_new = fs.fit_transform(X, y)\n",
    "    return X_new\n",
    "\n",
    "# feature selection\n",
    "def select_featuresPCA(X, y, n_comp):\n",
    "    fs = PCA(n_components=n_comp)\n",
    "    X_new = fs.fit_transform(X, y)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a voting ensemble of models\n",
    "def get_voting():\n",
    "    # define the base models\n",
    "    models = list()\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeRegressor()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('MLP', MLPClassifier()))\n",
    "    models.append(('DT', DecisionTreeClassifier()))\n",
    "    models.append(('RF', RandomForestClassifier()))\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('SVM', svm.SVC()))\n",
    "    models.append(('AdaBoost', AdaBoostClassifier()))\n",
    "    models.append(('GradientBoosting', GradientBoostingClassifier()))\n",
    "    models.append(('XGB', XGBClassifier()))\n",
    "    # define the voting ensemble\n",
    "    ensemble = VotingClassifier(estimators=models, voting='soft')\n",
    "    return ensemble\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = list()\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeRegressor()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('MLP', MLPClassifier()))\n",
    "    models.append(('DT', DecisionTreeClassifier()))\n",
    "    models.append(('RF', RandomForestClassifier()))\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('SVM', svm.SVC()))\n",
    "    models.append(('AdaBoost', AdaBoostClassifier()))\n",
    "    models.append(('GradientBoosting', GradientBoostingClassifier()))\n",
    "    models.append(('XGB', XGBClassifier()))\n",
    "    models.append(('soft_voting', get_voting()))\n",
    "    return models\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models:\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of a model fit using mutual information input features\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('MLP', MLPClassifier()))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('SVM', svm.SVC()))\n",
    "models.append(('AdaBoost', AdaBoostClassifier()))\n",
    "models.append(('GradientBoosting', GradientBoostingClassifier()))\n",
    "models.append(('XGB', XGBClassifier()))\n",
    "\n",
    "\n",
    "for name, model in models:\n",
    "    print('asd')\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    X_s = select_featuresMutual(X, y)\n",
    "    cv_results = cross_val_score(model, X_s, y, cv=kfold, scoring='accuracy')\n",
    "    print('%s: %s: %f (%f)' % ('Mutual-info', name, cv_results.mean(), cv_results.std()))\n",
    "    X_s = select_featuresPCA(X, y, 25)\n",
    "    cv_results = cross_val_score(model, X_s, y, cv=kfold, scoring='accuracy')\n",
    "    print('%s: %s: %f (%f)' % ('PCA', name, cv_results.mean(), cv_results.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
