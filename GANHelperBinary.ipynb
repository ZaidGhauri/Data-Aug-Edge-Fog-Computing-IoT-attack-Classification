{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GAN_171103\n",
    "import importlib\n",
    "importlib.reload(GAN_171103) # For reloading after making changes\n",
    "from GAN_171103 import *\n",
    "\n",
    "# Let's look at some of the generated data\n",
    "# First create the networks locally and load the weights\n",
    "\n",
    "\n",
    "def Initialize_Helper(data2):\n",
    "     # 32 # needs to be ~data_dim\n",
    "    base_n_count = 128 # 128\n",
    "    \n",
    "    nb_steps = 500 + 1 # 50000 # Add one for logging of the last interval\n",
    "    batch_size = 128 # 64\n",
    "    \n",
    "    k_d = 1  # number of critic network updates per adversarial training step\n",
    "    k_g = 1  # number of generator network updates per adversarial training step\n",
    "    critic_pre_train_steps = 100 # 100  # number of steps to pre-train the critic before starting adversarial training\n",
    "    log_interval = 100 # 100  # interval (in steps) at which to log loss summaries and save plots of image samples to disc\n",
    "    learning_rate = 5e-4 # 5e-5\n",
    "    generator_model_path, discriminator_model_path, loss_pickle_path = None, None, None\n",
    "    \n",
    "    # show = False\n",
    "    show = True \n",
    "    \n",
    "    # train = create_toy_spiral_df(1000)\n",
    "    # train = create_toy_df(n=1000,n_dim=2,n_classes=4,seed=0)\n",
    "    train = data2.copy().reset_index(drop=True) # fraud only with labels from classification\n",
    "    \n",
    "    \n",
    "    # train = pd.get_dummies(train, columns=['Class'], prefix='Class', drop_first=True)\n",
    "    label_cols = [ i for i in train.columns if 'Label' in i ]\n",
    "    data_cols = [ i for i in train.columns if i not in label_cols ]\n",
    "    train[ data_cols ] = train[ data_cols ] / 10 # scale to random noise size, one less thing to learn\n",
    "    train_no_label = train[ data_cols ]\n",
    "    \n",
    "    \n",
    "    rand_dim = len(data_cols)\n",
    "    \n",
    "    \n",
    "    %%time\n",
    "    \n",
    "    # Training the vanilla GAN and CGAN architectures\n",
    "    \n",
    "    k_d = 1  # number of critic network updates per adversarial training step\n",
    "    learning_rate = 5e-4 # 5e-5\n",
    "    arguments = [rand_dim, nb_steps, batch_size, \n",
    "                 k_d, k_g, critic_pre_train_steps, log_interval, learning_rate, base_n_count,\n",
    "                data_dir, generator_model_path, discriminator_model_path, loss_pickle_path, show ]\n",
    "    \n",
    "    adversarial_training_GAN(arguments, train_no_label, data_cols ) # GAN\n",
    "    #adversarial_training_GAN(arguments, train, data_cols=data_cols, label_cols=label_cols ) # CGAN\n",
    "    \n",
    "def Initialize_discriminator(data2):\n",
    "    seed = 17\n",
    "    \n",
    "    train = data2.copy().reset_index(drop=True) # fraud only with labels from classification\n",
    "    \n",
    "    # train = pd.get_dummies(train, columns=['Class'], prefix='Class', drop_first=True)\n",
    "    label_cols = [ train.columns[-1]  ]\n",
    "    data_cols = [ i for i in train.columns if i not in label_cols ]\n",
    "    #train[ data_cols ] = train[ data_cols ] / 10 # scale to random noise size, one less thing to learn\n",
    "    train_no_label = train[ data_cols ]\n",
    "    \n",
    "    data_dim = len(data_cols)\n",
    "    label_dim = len(label_cols)\n",
    "    #if label_dim > 0: with_class = True\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # define network models\n",
    "    \n",
    "    generator_model, discriminator_model, combined_model = define_models_GAN(rand_dim, data_dim, base_n_count)\n",
    "    generator_model.load_weights('cache/GAN_generator_model_weights_step_500.h5')\n",
    "    test_size = len(train) # Equal to all of the fraud cases\n",
    "    \n",
    "    x = get_data_batch(train_no_label, test_size, seed=3)\n",
    "    z = np.random.normal(size=(test_size, rand_dim))\n",
    "    g_z = generator_model.predict(z)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # check data\n",
    "    # =============================================================================\n",
    "    df2=pd.DataFrame(np.rint(np.abs(g_z)),columns=df.columns[:-1])\n",
    "    df2=df2.astype(int)\n",
    "    return df2\n",
    "\n",
    "\n",
    "def define_models_GAN(rand_dim, data_dim, base_n_count, type=None):\n",
    "    generator_input_tensor = layers.Input(shape=(rand_dim, ))\n",
    "    generated_image_tensor = generator_network(generator_input_tensor, data_dim, base_n_count)\n",
    "\n",
    "    generated_or_real_image_tensor = layers.Input(shape=(data_dim,))\n",
    "    \n",
    "    if type == 'Wasserstein':\n",
    "        discriminator_output = critic_network(generated_or_real_image_tensor, data_dim, base_n_count)\n",
    "    else:\n",
    "        discriminator_output = discriminator_network(generated_or_real_image_tensor, data_dim, base_n_count)\n",
    "\n",
    "    generator_model = models.Model(inputs=[generator_input_tensor], outputs=[generated_image_tensor], name='generator')\n",
    "    discriminator_model = models.Model(inputs=[generated_or_real_image_tensor],\n",
    "                                       outputs=[discriminator_output],\n",
    "                                       name='discriminator')\n",
    "\n",
    "    combined_output = discriminator_model(generator_model(generator_input_tensor))\n",
    "    combined_model = models.Model(inputs=[generator_input_tensor], outputs=[combined_output], name='combined')\n",
    "    \n",
    "    return generator_model, discriminator_model, combined_model\n",
    "\n",
    "def generate_data_gan(n_samples,Xdf,path_model,rand_dim=32):\n",
    "    generator_model, discriminator_model, combined_model= define_models_GAN(rand_dim,data_dim=len(Xdf.columns),base_n_count=128)\n",
    "    generator_model.load_weights(path_model)\n",
    "    z = np.random.normal(size=(n_samples, rand_dim))\n",
    "    g_z = generator_model.predict(z)\n",
    "    \n",
    "    newdf = pd.DataFrame()\n",
    "    for i, col in enumerate(Xdf.columns):\n",
    "\n",
    "        if Xdf[col].dtype == 'int32' or Xdf[col].dtype == 'int64':\n",
    "            newdf[col] = np.rint(np.abs(g_z[:,i])).astype(int)    \n",
    "        elif Xdf[col].dtype == 'float_':\n",
    "            newdf[col] = np.abs(g_z[:,i])\n",
    "    return newdf\n",
    "\n",
    "\n",
    "\n",
    "def callDataGAN(data2, data_gen_dim):\n",
    "    #rand_dim = 32 \n",
    "    #base_n_count = 128\n",
    "    #data_dim = len(data_cols) \n",
    "    #'nursery','mushroom'\n",
    "    \n",
    "    \n",
    "    Initialize_Helper(data2)\n",
    "    Initialize_discriminator(data2)\n",
    "    \n",
    "    #data11 = 119698\n",
    "    #data12-False = 62% of data2 = 74213\n",
    "    #data12-True  = 38% of data2 = 45485\n",
    "    # data2 False\n",
    "    n_samples=data_gen_dim\n",
    "    \n",
    "    # data2 True\n",
    "    #n_samples=45485\n",
    "    \n",
    "    Xdf=data2.iloc[:,:-1]    \n",
    "    path_model='cache/GAN_generator_model_weights_step_500.h5'\n",
    "    \n",
    "    data12=generate_data_gan(n_samples,Xdf,path_model,rand_dim=len(data_cols))  \n",
    "    return data12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
