{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "\n",
    "ioTInTData = pd.read_csv(\"data/IoTID20/IoT_Network_Intrusion_Dataset_Category.csv\" , low_memory=False)\n",
    "#ioTInTData = ioTInTData[ioTInTData['Cat'] != 'Mirai']\n",
    "ioTInTData = ioTInTData.iloc[:,:81]\n",
    "ioTInTData = ioTInTData.drop(['Label'],axis=1) \n",
    "print('ioTInTData', ioTInTData.shape)\n",
    "\n",
    "#ioTInTData = ioTInTData.fillna(np.nan, inplace=True)\n",
    "ioTInTData.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "ioTInTData.dropna(inplace=True)\n",
    "ioTInTData.fillna(0)\n",
    "ioTInTData.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def readDataSets():\n",
    "    dataSetFridge = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/cat/IoT_Fridge.csv')\n",
    "    dataSetGarageDoor = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/cat/IoT_Garage_Door.csv')\n",
    "    dataSetGPS = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/cat/IoT_GPS_Tracker.csv')\n",
    "    dataSetModbus = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/cat/IoT_Modbus.csv')\n",
    "    dataSetMotionLight = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/cat/IoT_Motion_Light.csv')\n",
    "    dataSetThermostat = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/cat/IoT_Thermostat.csv')\n",
    "    dataSetWeahter = pd.read_csv(filepath_or_buffer = 'data/ToNIoT/cat/IoT_Weather.csv')\n",
    "\n",
    "    dataSetFridge['temp_condition'] = dataSetFridge['temp_condition'].str.strip()\n",
    "    dataSetGarageDoor['door_state'] = dataSetGarageDoor['door_state'].str.strip()\n",
    "    dataSetMotionLight['light_status'] = dataSetMotionLight['light_status'].str.strip()\n",
    "    dataSetRawLoad = pd.concat([dataSetFridge, dataSetGarageDoor, dataSetGPS, dataSetModbus, dataSetMotionLight, dataSetThermostat, dataSetWeahter])\n",
    "    \n",
    "    dataSetRawLoad = dataSetRawLoad[dataSetRawLoad['type'] != 'xss']\n",
    "    dataSetRawLoad = dataSetRawLoad[dataSetRawLoad['type'] != 'normal']\n",
    "    dataSetRawLoad = dataSetRawLoad[dataSetRawLoad['type'] != 'ransomware']\n",
    "    dataSetRawLoad = dataSetRawLoad[dataSetRawLoad['type'] != 'injection']\n",
    "    dataSetRawLoad = dataSetRawLoad[dataSetRawLoad['type'] != 'backdoor']\n",
    "\n",
    "    return dataSetRawLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ioTInTData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = readDataSets()\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "categorical_features = ['door_state','sphone_signal', 'light_status','temp_condition']\n",
    "quantitative_features = ['FC1_Read_Input_Register','FC2_Read_Discrete_Value','FC3_Read_Holding_Register','FC4_Read_Coil','current_temperature',\n",
    "                        'fridge_temperature','humidity','latitude','FC4_Read_Coil','longitude',\n",
    "                        'motion_status','pressure','temperature','thermostat_status']\n",
    "features = categorical_features + quantitative_features\n",
    "\n",
    "\n",
    "def datapreprocessingShuffle(data):\n",
    "    scaler = StandardScaler()            \n",
    "    # Feature scaling\n",
    "    for i in data.columns:\n",
    "        if(data[i].name !='Cat'):\n",
    "            data[i] = scaler.fit_transform(data[[i]])\n",
    "        else:\n",
    "            data[i]=data[i]\n",
    "            \n",
    "    data = shuffle(data).reset_index(drop=True) \n",
    "    return data\n",
    "\n",
    "\n",
    "def datapreprocessingShuffle2(data):\n",
    "               \n",
    "    # Feature scaling\n",
    "    for i in quantitative_features :\n",
    "            scaler = StandardScaler()\n",
    "            data[i] = scaler.fit_transform(data[[i]])\n",
    "            \n",
    "    # Encoding categorical features    \n",
    "    for i in categorical_features :\n",
    "        labelencoder=LabelEncoder()\n",
    "        data[i]=labelencoder.fit_transform(data[i])   \n",
    "    \n",
    "    data = shuffle(data).reset_index(drop=True) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing datset\n",
    "datacopy1 = ioTInTData.copy()\n",
    "datacopy2 = df.copy()\n",
    "datacopy1['Cat'] = datacopy1.iloc[:,79:80].replace('DoS', '0').replace('Scan', '1').replace('MITM ARP Spoofing', '2').astype('int')\n",
    "data1 = datapreprocessingShuffle(datacopy1)\n",
    "\n",
    "data2 = datacopy2.rename(columns={\"type\": \"Cat\"})\n",
    "data2['Cat'] = data2.iloc[:,17:18].replace('ddos', '0').replace('scanning', '1').replace('password', '3').astype('int')\n",
    "data2 = datapreprocessingShuffle2(data2) \n",
    "print(data1.shape)\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, Concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class cGAN():\n",
    "    def __init__(self):\n",
    "        self.latent_dim = 120\n",
    "        self.out_shape = 120\n",
    "        self.num_classes = 2\n",
    "        self.clip_value = 0.01\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        #optimizer = RMSprop(lr=0.00005)\n",
    "\n",
    "        # build discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        # build generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # generating new data samples\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        gen_samples = self.generator([noise, label])\n",
    "\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # passing gen samples through disc. \n",
    "        valid = self.discriminator([gen_samples, label])\n",
    "\n",
    "        # combining both models\n",
    "        self.combined = Model([noise, label], valid)\n",
    "        self.combined.compile(loss=['binary_crossentropy'],\n",
    "                              optimizer=optimizer,\n",
    "                             metrics=['accuracy'])\n",
    "        self.combined.summary()\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self):\n",
    "        init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128, input_dim=self.latent_dim))\n",
    "        #model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        model.add(Dense(256))\n",
    "        #model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        model.add(Dense(512))\n",
    "        #model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        model.add(Dense(self.out_shape, activation='tanh'))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "        \n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        gen_sample = model(model_input)\n",
    "\n",
    "        return Model([noise, label], gen_sample, name=\"Generator\")\n",
    "\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.out_shape, kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Dense(256, kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        model.add(Dense(128, kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "        \n",
    "        gen_sample = Input(shape=(self.out_shape,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.out_shape)(label))\n",
    "\n",
    "        model_input = multiply([gen_sample, label_embedding])\n",
    "        validity = model(model_input)\n",
    "\n",
    "        return Model(inputs=[gen_sample, label], outputs=validity, name=\"Discriminator\")\n",
    "\n",
    "\n",
    "    def train(self, X_train, y_train, pos_index, neg_index, epochs, batch_size=32, sample_interval=50):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            #  Train Discriminator with 8 sample from postivite class and rest with negative class\n",
    "            idx1 = np.random.choice(pos_index, 8)\n",
    "            idx0 = np.random.choice(neg_index, batch_size-8)\n",
    "            idx = np.concatenate((idx1, idx0))\n",
    "            samples, labels = X_train[idx], y_train[idx]\n",
    "            samples, labels = shuffle(samples, labels)\n",
    "            # Sample noise as generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_samples = self.generator.predict([noise, labels])\n",
    "\n",
    "            # label smoothing\n",
    "            if epoch < epochs//1.5:\n",
    "                valid_smooth = (valid+0.1)-(np.random.random(valid.shape)*0.1)\n",
    "                fake_smooth = (fake-0.1)+(np.random.random(fake.shape)*0.1)\n",
    "            else:\n",
    "                valid_smooth = valid \n",
    "                fake_smooth = fake\n",
    "                \n",
    "            # Train the discriminator\n",
    "            self.discriminator.trainable = True\n",
    "            d_loss_real = self.discriminator.train_on_batch([samples, labels], valid_smooth)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([gen_samples, labels], fake_smooth)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train Generator\n",
    "            # Condition on labels\n",
    "            self.discriminator.trainable = False\n",
    "            sampled_labels = np.random.randint(0, 2, batch_size).reshape(-1, 1)\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            if (epoch+1)%sample_interval==0:\n",
    "                print (f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}] [G loss: {g_loss}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.copy()\n",
    "data2_xtrain = data2.drop(\"Cat\",1)\n",
    "data2_ytrain = data2['Cat']\n",
    "data2_ytrain = np.array(data2_ytrain)\n",
    "data2_ytrain = data2_ytrain.reshape(-1,1)\n",
    "pos_index = np.where(data2_ytrain==1)[0]\n",
    "neg_index = np.where(data2_ytrain==0)[0]\n",
    "\n",
    "#(92172, 80)\n",
    "#(63973, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan.train(data2_xtrain, data2_ytrain, pos_index, neg_index, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating new samples\n",
    "noise = np.random.normal(0, 1, (63973, 18))\n",
    "sampled_labels = np.ones(63973).reshape(-1, 1)\n",
    "\n",
    "gen_samples = cgan.generator.predict([noise, sampled_labels])\n",
    "gen_samples = x_scaler.inverse_transform(gen_samples)\n",
    "print(gen_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na with mean for ToN_IoT dataset\n",
    "data2 = data2.fillna(data2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na with mean for ToN_IoT dataset\n",
    "fData = pd.concat([data1, data2])\n",
    "fData = fData.fillna(fData.mean())\n",
    "fData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(fData['Cat'].value_counts())[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(fData['Cat'].value_counts())[:]\n",
    "#matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "carrier_count = fData['Cat'].value_counts()\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.barplot(carrier_count.index, carrier_count.values, alpha=0.9)\n",
    "plt.title('Frequency Distribution of Cat')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Cat', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fData['Cat']\n",
    "X = fData.drop(['Cat'],axis=1) \n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# on this distribution. \n",
    "sc = MinMaxScaler()\n",
    "X_std =  sc.fit_transform(X)\n",
    "\n",
    "cov_matrix = np.cov(X_std.T)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "# Make a set of (eigenvalue, eigenvector) pairs:\n",
    "eig_pairs = [(eigenvalues[index], eigenvectors[:,index]) for index in range(len(eigenvalues))]\n",
    "# Sort the (eigenvalue, eigenvector) pairs from highest to lowest with respect to eigenvalue\n",
    "#eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "#print(eig_pairs)\n",
    "# Extract the descending ordered eigenvalues and eigenvectors\n",
    "eigvalues_sorted = [eig_pairs[index][0] for index in range(len(eigenvalues))]\n",
    "eigvectors_sorted = [eig_pairs[index][1] for index in range(len(eigenvalues))]\n",
    "# Let's confirm our sorting worked, print out eigenvalues\n",
    "#print('Eigenvalues in descending order: \\n%s' %eigvalues_sorted)\n",
    "\n",
    "tot = sum(eigenvalues)\n",
    "var_explained = [(i / tot) for i in sorted(eigenvalues, reverse=True)]  # an array of variance explained by each \n",
    "# eigen vector... there will be 18 entries as there are 18 eigen vectors)\n",
    "cum_var_exp = np.cumsum(var_explained)  # an array of cumulative variance. There will be 18 entries with 18 th entry \n",
    "# cumulative reaching almost 100%\n",
    "\n",
    "plt.bar(range(1,97), var_explained, alpha=0.5, align='center', label='individual explained variance')\n",
    "plt.step(range(1,97),cum_var_exp, where= 'mid', label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(input):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(input)\n",
    "    value = le.transform(input)\n",
    "    return value\n",
    "\n",
    "# feature selection\n",
    "def select_featuresMutual(X_train, y_train, X_test):\n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k=4)\n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs\n",
    "\n",
    "# feature selection\n",
    "def select_featuresChi(X_train, y_train, X_test):\n",
    "    fs = SelectKBest(score_func=chi2, k=4)\n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs\n",
    "\n",
    "# feature selection\n",
    "def select_featuresPCA(X_train, y_train, X_test, n_comp):\n",
    "    fs = PCA(n_components=n_comp)\n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a voting ensemble of models\n",
    "def get_voting():\n",
    "    # define the base models\n",
    "    models = list()\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeRegressor()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('MLP', MLPClassifier()))\n",
    "    models.append(('DT', DecisionTreeClassifier()))\n",
    "    models.append(('RF', RandomForestClassifier()))\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('SVM', svm.SVC()))\n",
    "    models.append(('AdaBoost', AdaBoostClassifier()))\n",
    "    models.append(('GradientBoosting', GradientBoostingClassifier()))\n",
    "    models.append(('XGB', XGBClassifier()))\n",
    "    # define the voting ensemble\n",
    "    ensemble = VotingClassifier(estimators=models, voting='soft')\n",
    "    return ensemble\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = list()\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeRegressor()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('MLP', MLPClassifier()))\n",
    "    models.append(('DT', DecisionTreeClassifier()))\n",
    "    models.append(('RF', RandomForestClassifier()))\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('SVM', svm.SVC()))\n",
    "    models.append(('AdaBoost', AdaBoostClassifier()))\n",
    "    models.append(('GradientBoosting', GradientBoostingClassifier()))\n",
    "    models.append(('XGB', XGBClassifier()))\n",
    "    models.append(('soft_voting', get_voting()))\n",
    "    return models\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models:\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of a model fit using mutual information input features\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('MLP', MLPClassifier()))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('SVM', svm.SVC()))\n",
    "models.append(('AdaBoost', AdaBoostClassifier()))\n",
    "models.append(('GradientBoosting', GradientBoostingClassifier()))\n",
    "models.append(('XGB', XGBClassifier()))\n",
    "\n",
    "for name, model in models:\n",
    "    print(model)\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    result = next(kfold.split(X,y), None)\n",
    "    x_train = X.iloc[result[0]]\n",
    "    x_test = X.iloc[result[1]]\n",
    "    y_train = y.iloc[result[0]]\n",
    "    y_test = y.iloc[result[1]]\n",
    "    # feature selection\n",
    "    X_train_fs, X_test_fs = select_featuresMutual(x_train, y_train, x_test)\n",
    "    model.fit(X_train_fs, y_train)\n",
    "    # evaluate the model\n",
    "    y_pred = model.predict(X_test_fs)\n",
    "    print('Mutual', metrics.classification_report(y_test, y_pred, target_names=['DoS', 'scanning', 'MITM ARP Spoofing', 'password']))\n",
    "    #\n",
    "    #PCA\n",
    "    # feature selection\n",
    "    X_train_fs, X_test_fs = select_featuresPCA(x_train, y_train, x_test, 20)\n",
    "    model.fit(X_train_fs, y_train)\n",
    "    # evaluate the model\n",
    "    y_pred = model.predict(X_test_fs)\n",
    "    print('PCA', metrics.classification_report(y_test, y_pred, target_names=['DoS', 'scanning', 'MITM ARP Spoofing', 'password']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
